{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1457b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim_metric\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c3852b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d2cc8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 24 23:26:58 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 572.60                 Driver Version: 572.60         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   41C    P8              2W /  140W |    1033MiB /   6141MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           20132      C   ...s\\Python\\Python312\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5eb75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Dataset\n",
    "    data_root = 'DIV2K'  # Adjust to your path\n",
    "    scale = 2  # Upscaling factor (2, 3, or 4)\n",
    "    patch_size = 128  # HR patch size for training\n",
    "    aug_probability = 0.5  # Probability of applying augmentations\n",
    "\n",
    "    # Model\n",
    "    base_channels = 64\n",
    "    use_channel_attention = True\n",
    "    use_spatial_attention = True\n",
    "    num_residual_blocks = 16\n",
    "\n",
    "    # Training\n",
    "    batch_size = 16\n",
    "    num_workers = 0 # Adjust based on your system\n",
    "    lr = 2e-4\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-4\n",
    "    num_epochs = 100\n",
    "    warmup_epochs = 5\n",
    "\n",
    "    # Loss\n",
    "    l1_weight = 1.0\n",
    "    perceptual_weight = 0.1\n",
    "    freq_loss_weight = 0.05\n",
    "\n",
    "    # Logging\n",
    "    save_dir = 'results'\n",
    "    checkpoint_interval = 5\n",
    "    log_interval = 100\n",
    "    use_wandb = False  # Set to True if you want to use Weights & Biases\n",
    "\n",
    "    # Mixed precision\n",
    "    use_amp = True\n",
    "\n",
    "# Initialize config\n",
    "cfg = Config()\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize wandb if enabled\n",
    "if cfg.use_wandb:\n",
    "    # Make sure to login to wandb first if running locally\n",
    "    try:\n",
    "        wandb.init(project=\"hybrid-freq-spatial-image-restoration\", config=vars(cfg))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not initialize wandb: {e}. Set cfg.use_wandb=False to disable.\")\n",
    "        cfg.use_wandb = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63ae97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2KDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DIV2K with both LR and HR images\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, split=\"train\", scale=2, patch_size=128, augment=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Directory with DIV2K dataset\n",
    "            split: 'train' or 'valid'\n",
    "            scale: Downscaling factor (2, 3, or 4)\n",
    "            patch_size: Size of cropped patches for training\n",
    "            augment: Whether to apply data augmentation\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "        self.augment = augment and split == \"train\"\n",
    "\n",
    "        # Define paths\n",
    "        self.hr_dir = os.path.join(root_dir, f\"DIV2K_{split}_HR\")\n",
    "        self.lr_dir = os.path.join(root_dir, f\"DIV2K_{split}_LR_bicubic\", f\"X{scale}\")\n",
    "\n",
    "        # Get all HR images\n",
    "        self.hr_images = sorted(glob.glob(os.path.join(self.hr_dir, \"*.png\")))\n",
    "\n",
    "        # Make sure we have files\n",
    "        if len(self.hr_images) == 0:\n",
    "            raise RuntimeError(f\"No images found in {self.hr_dir}. Please check the path.\")\n",
    "\n",
    "        # Basic transforms\n",
    "        self.to_tensor = transforms.ToTensor() # Converts PIL image [0, 255] to Tensor [0, 1]\n",
    "\n",
    "        print(f\"Loaded {len(self.hr_images)} images for {split}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get HR image path\n",
    "        hr_path = self.hr_images[idx]\n",
    "\n",
    "        # Extract image ID from filename (assuming format like \"0001.png\")\n",
    "        img_id = os.path.basename(hr_path).split('.')[0]\n",
    "\n",
    "        # Construct LR image path (e.g., \"0001x2.png\")\n",
    "        lr_path = os.path.join(self.lr_dir, f\"{img_id}x{self.scale}.png\")\n",
    "\n",
    "        # Load images\n",
    "        try:\n",
    "            hr_img = Image.open(hr_path).convert('RGB')\n",
    "            lr_img = Image.open(lr_path).convert('RGB')\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error loading image: {e}\")\n",
    "            # Return dummy data or handle appropriately\n",
    "            # For simplicity, we'll raise the error here\n",
    "            raise e\n",
    "\n",
    "        # Random crop for training\n",
    "        if self.split == \"train\":\n",
    "            # Get dimensions\n",
    "            hr_width, hr_height = hr_img.size\n",
    "\n",
    "            # Randomly select patch\n",
    "            hr_patch_size = self.patch_size\n",
    "            lr_patch_size = hr_patch_size // self.scale\n",
    "\n",
    "            # Ensure we can extract a patch of the desired size\n",
    "            if hr_width < hr_patch_size or hr_height < hr_patch_size:\n",
    "                # If image is smaller than patch_size, resize it (might affect quality)\n",
    "                # Consider skipping smaller images or using different padding/cropping strategy\n",
    "                hr_img = hr_img.resize((hr_patch_size, hr_patch_size), Image.BICUBIC)\n",
    "                lr_img = lr_img.resize((lr_patch_size, lr_patch_size), Image.BICUBIC)\n",
    "                hr_x, hr_y = 0, 0\n",
    "                lr_x, lr_y = 0, 0\n",
    "            else:\n",
    "                # Random crop\n",
    "                hr_x = random.randint(0, hr_width - hr_patch_size)\n",
    "                hr_y = random.randint(0, hr_height - hr_patch_size)\n",
    "\n",
    "                lr_x = hr_x // self.scale\n",
    "                lr_y = hr_y // self.scale\n",
    "\n",
    "                hr_img = hr_img.crop((hr_x, hr_y, hr_x + hr_patch_size, hr_y + hr_patch_size))\n",
    "                lr_img = lr_img.crop((lr_x, lr_y, lr_x + lr_patch_size, lr_y + lr_patch_size))\n",
    "\n",
    "        # Apply augmentations for training\n",
    "        if self.augment and random.random() < cfg.aug_probability:\n",
    "            # Random horizontal flip\n",
    "            if random.random() < 0.5:\n",
    "                hr_img = hr_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                lr_img = lr_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "            # Random vertical flip\n",
    "            if random.random() < 0.5:\n",
    "                hr_img = hr_img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                lr_img = lr_img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "            # Random rotation (90, 180, 270 degrees)\n",
    "            rot_factor = random.choice([0, 1, 2, 3])\n",
    "            if rot_factor > 0:\n",
    "                hr_img = hr_img.rotate(90 * rot_factor)\n",
    "                lr_img = lr_img.rotate(90 * rot_factor)\n",
    "\n",
    "        # Apply transforms\n",
    "        hr_tensor = self.to_tensor(hr_img)\n",
    "        lr_tensor = self.to_tensor(lr_img)\n",
    "\n",
    "        return {'lr': lr_tensor, 'hr': hr_tensor, 'idx': idx, 'hr_path': hr_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f981b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel Attention Module\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        # Global average pooling\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # Global max pooling\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # Shared MLP\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // reduction, 1, bias=False), # 1x1 conv for reduction\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel // reduction, channel, 1, bias=False)  # 1x1 conv for expansion\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation to get attention weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.conv(self.avg_pool(x)) # Pass avg pooled features through MLP\n",
    "        max_out = self.conv(self.max_pool(x)) # Pass max pooled features through MLP\n",
    "        attention_weights = self.sigmoid(avg_out + max_out) # Combine and activate\n",
    "        return attention_weights * x # Apply attention weights to input feature map\n",
    "\n",
    "# Spatial Attention Module\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        # Convolution layer to process concatenated pooled features\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False) # 2 input channels (avg + max)\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation to get attention map\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True) # Average pooling across channels\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True) # Max pooling across channels\n",
    "        y = torch.cat([avg_out, max_out], dim=1) # Concatenate pooled features\n",
    "        attention_map = self.sigmoid(self.conv(y)) # Generate spatial attention map\n",
    "        return attention_map * x # Apply attention map to input feature map\n",
    "\n",
    "# Residual Channel Attention Block (RCAB)\n",
    "class RCAB(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, use_ca=True, use_sa=False):\n",
    "        super(RCAB, self).__init__()\n",
    "        self.use_ca = use_ca\n",
    "        self.use_sa = use_sa\n",
    "\n",
    "        # Main convolutional path\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # Using LeakyReLU\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        # Optional Channel Attention\n",
    "        if use_ca:\n",
    "            self.ca = ChannelAttention(channels, reduction)\n",
    "        # Optional Spatial Attention\n",
    "        if use_sa:\n",
    "            self.sa = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x) # Pass through main conv layers\n",
    "        if self.use_ca:\n",
    "            res = self.ca(res) # Apply channel attention\n",
    "        if self.use_sa:\n",
    "            res = self.sa(res) # Apply spatial attention\n",
    "        return res + x # Residual connection\n",
    "\n",
    "# Residual Group (contains multiple RCABs)\n",
    "class ResidualGroup(nn.Module):\n",
    "    def __init__(self, channels, n_blocks=8, use_ca=True, use_sa=False):\n",
    "        super(ResidualGroup, self).__init__()\n",
    "\n",
    "        # Stack multiple RCABs\n",
    "        body = [RCAB(channels, use_ca=use_ca, use_sa=use_sa) for _ in range(n_blocks)]\n",
    "        self.body = nn.Sequential(*body)\n",
    "\n",
    "        # Final convolution within the group\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x) # Pass through all RCABs\n",
    "        res = self.conv(res) # Final convolution\n",
    "        return res + x # Residual connection for the whole group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89cb3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic convolutional block with PReLU activation and optional instance normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_norm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
    "        if use_norm:\n",
    "            # Using InstanceNorm instead of BatchNorm for potentially better style transfer/SR tasks\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.PReLU(num_parameters=out_channels)) # Using PReLU activation\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class SpatialBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatial branch using residual groups for feature extraction from pixel data\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=64, base_channels=64, num_blocks=8):\n",
    "        super(SpatialBranch, self).__init__()\n",
    "\n",
    "        # Initial feature extraction (adjusts input channels if needed)\n",
    "        self.conv_first = nn.Conv2d(in_channels, base_channels, 3, 1, 1)\n",
    "\n",
    "        # Residual Groups with channel and spatial attention\n",
    "        # Dividing the total residual blocks between groups\n",
    "        self.residual_groups = nn.ModuleList([\n",
    "            ResidualGroup(base_channels, n_blocks=num_blocks,\n",
    "                         use_ca=cfg.use_channel_attention,\n",
    "                         use_sa=cfg.use_spatial_attention)\n",
    "            for _ in range(2) # Using 2 residual groups\n",
    "        ])\n",
    "\n",
    "        # Final feature processing convolution within the branch\n",
    "        self.conv_last = nn.Conv2d(base_channels, base_channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial feature extraction\n",
    "        x = F.leaky_relu(self.conv_first(x), 0.2, inplace=True) # Apply activation after first conv\n",
    "        residual = x # Store for global residual connection\n",
    "\n",
    "        # Pass through residual groups\n",
    "        for rg in self.residual_groups:\n",
    "            x = rg(x)\n",
    "\n",
    "        # Final processing convolution\n",
    "        x = self.conv_last(x)\n",
    "\n",
    "        # Global residual connection across the spatial branch\n",
    "        return x + residual\n",
    "\n",
    "class ComplexConv2d(nn.Module):\n",
    "    \"\"\"Complex-valued convolutional layer\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ComplexConv2d, self).__init__()\n",
    "        # Convolution for the real part of the input\n",
    "        self.conv_real = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # Convolution for the imaginary part of the input\n",
    "        self.conv_imag = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is a tuple of (real_tensor, imag_tensor)\n",
    "        real, imag = x\n",
    "        # Complex convolution formula: (a+bi)*(c+di) = (ac-bd) + (ad+bc)i\n",
    "        # Output real part: conv_real(real) - conv_imag(imag)\n",
    "        real_out = self.conv_real(real) - self.conv_imag(imag)\n",
    "        # Output imaginary part: conv_real(imag) + conv_imag(real)\n",
    "        imag_out = self.conv_real(imag) + self.conv_imag(real)\n",
    "        return (real_out, imag_out)\n",
    "\n",
    "class ComplexBatchNorm2d(nn.Module):\n",
    "    \"\"\"Complex batch normalization\"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        super(ComplexBatchNorm2d, self).__init__()\n",
    "        # Separate BatchNorm for real and imaginary parts\n",
    "        self.bn_real = nn.BatchNorm2d(num_features)\n",
    "        self.bn_imag = nn.BatchNorm2d(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        real, imag = x\n",
    "        # Apply BatchNorm independently to real and imaginary tensors\n",
    "        return (self.bn_real(real), self.bn_imag(imag))\n",
    "\n",
    "class ComplexReLU(nn.Module):\n",
    "    \"\"\"Complex ReLU activation (applies ReLU independently to real and imag parts)\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ComplexReLU, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        real, imag = x\n",
    "        # Apply ReLU to real and imaginary parts separately\n",
    "        return (self.relu(real), self.relu(imag))\n",
    "\n",
    "class ComplexConvBlock(nn.Module):\n",
    "    \"\"\"Complex convolutional block with normalization and activation\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ComplexConvBlock, self).__init__()\n",
    "        self.conv = ComplexConv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = ComplexBatchNorm2d(out_channels) # Use complex batch norm\n",
    "        self.relu = ComplexReLU() # Use complex ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce1fa7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced frequency branch that processes the image in frequency domain\n",
    "    Handles potential precision issues with FFT during validation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=64, base_channels=64): # Input to branch has base_channels\n",
    "        super(FrequencyBranch, self).__init__()\n",
    "\n",
    "        # Processing for real and imaginary components directly from FFT\n",
    "        self.complex_blocks = nn.ModuleList([\n",
    "            ComplexConvBlock(base_channels, base_channels) # Expects base_channels input\n",
    "            for _ in range(4) # Number of complex blocks\n",
    "        ])\n",
    "\n",
    "        # Magnitude processing path\n",
    "        self.magnitude_conv = nn.Sequential(\n",
    "            ConvBlock(in_channels, base_channels // 2), # Reduce channels to 32\n",
    "            ResidualGroup(base_channels // 2, n_blocks=4, # Process with 32 channels\n",
    "                         use_ca=cfg.use_channel_attention,\n",
    "                         use_sa=False)\n",
    "        )\n",
    "\n",
    "        # Phase processing path\n",
    "        self.phase_conv = nn.Sequential(\n",
    "            ConvBlock(in_channels, base_channels // 2), # Reduce channels to 32\n",
    "            ResidualGroup(base_channels // 2, n_blocks=4, # Process with 32 channels\n",
    "                         use_ca=cfg.use_channel_attention,\n",
    "                         use_sa=False)\n",
    "        )\n",
    "\n",
    "        # Convert combined magnitude/phase features back towards spatial domain representation\n",
    "        self.freq_to_spatial = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels, 3, 1, 1),\n",
    "            nn.InstanceNorm2d(base_channels),\n",
    "            nn.PReLU(num_parameters=base_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x has shape [B, C=base_channels, H, W]\n",
    "\n",
    "        # --- FIX: Cast to float32 before FFT ---\n",
    "        # Ensure FFT runs in full precision to avoid cuFFT half-precision limitations\n",
    "        # on non-power-of-two dimensions, especially during validation.\n",
    "        x_float32 = x.float()\n",
    "        # -----------------------------------------\n",
    "\n",
    "        # Apply 2D FFT to convert to frequency domain\n",
    "        x_fft = torch.fft.fft2(x_float32, norm='ortho') # Use the float32 version for FFT\n",
    "\n",
    "        # Split into real and imaginary parts\n",
    "        real = x_fft.real # Shape [B, C, H, W]\n",
    "        imag = x_fft.imag # Shape [B, C, H, W]\n",
    "\n",
    "        # Compute magnitude and phase\n",
    "        magnitude = torch.sqrt(real**2 + imag**2 + 1e-10)\n",
    "        phase = torch.atan2(imag, real + 1e-10)\n",
    "\n",
    "        # Apply log scaling to magnitude\n",
    "        log_magnitude = torch.log(magnitude + 1.0)\n",
    "\n",
    "        # Process magnitude path\n",
    "        # Note: log_magnitude might need casting back if subsequent layers expect AMP type\n",
    "        # However, ConvBlock likely handles mixed types due to autocast.\n",
    "        mag_features = self.magnitude_conv(log_magnitude) # Output shape [B, C/2, H, W]\n",
    "\n",
    "        # Process phase path\n",
    "        phase_features = self.phase_conv(phase) # Output shape [B, C/2, H, W]\n",
    "\n",
    "        # Process complex components directly using complex convolutions\n",
    "        # The ComplexConvBlock inputs (real, imag) derived from x_fft will be float32.\n",
    "        # The ComplexConv2d layers inside will perform calculations potentially\n",
    "        # using float16 due to autocast, but the input types are handled.\n",
    "        complex_input = (real, imag) # Tuple for complex layers (float32)\n",
    "        complex_out_features = complex_input\n",
    "        for block in self.complex_blocks:\n",
    "            complex_out_features = block(complex_out_features)\n",
    "        # complex_out_features is a tuple (real_feat, imag_feat)\n",
    "\n",
    "        # Combine processed magnitude and phase features\n",
    "        combined_mag_phase_features = torch.cat([mag_features, phase_features], dim=1) # Shape [B, C, H, W]\n",
    "\n",
    "        # Convert combined magnitude/phase features to spatial domain features\n",
    "        spatial_domain_freq_features = self.freq_to_spatial(combined_mag_phase_features) # Shape [B, C, H, W]\n",
    "\n",
    "        # Return the spatial-domain representation from mag/phase and the processed complex features\n",
    "        # The types of returned features will depend on the last operation within autocast context.\n",
    "        return spatial_domain_freq_features, complex_out_features\n",
    "\n",
    "\n",
    "class AttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced fusion module using channel and spatial attention to combine features\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=64):\n",
    "        super(AttentionFusion, self).__init__()\n",
    "\n",
    "        # Initial fusion using 1x1 convolution to reduce channels\n",
    "        self.conv1 = nn.Conv2d(channels * 2, channels, kernel_size=1, stride=1, padding=0) # Input is concatenation (2*channels)\n",
    "\n",
    "        # Channel attention on fused features\n",
    "        self.ca = ChannelAttention(channels)\n",
    "\n",
    "        # Spatial attention on fused features\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "        # Final processing block after attention\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm = nn.InstanceNorm2d(channels)\n",
    "        self.prelu = nn.PReLU(num_parameters=channels)\n",
    "\n",
    "    def forward(self, spatial_features, freq_features):\n",
    "        # Concatenate features from both branches along the channel dimension\n",
    "        concat_features = torch.cat([spatial_features, freq_features], dim=1) # Shape [B, 2*C, H, W]\n",
    "\n",
    "        # Initial fusion and channel reduction\n",
    "        fused = F.leaky_relu(self.conv1(concat_features), 0.2, inplace=True) # Shape [B, C, H, W]\n",
    "\n",
    "        # Apply attention mechanisms sequentially\n",
    "        fused_ca = self.ca(fused)\n",
    "        fused_sa = self.sa(fused_ca) # Apply spatial attention after channel attention\n",
    "\n",
    "        # Final processing\n",
    "        fused_out = self.conv2(fused_sa)\n",
    "        fused_out = self.norm(fused_out)\n",
    "        fused_out = self.prelu(fused_out)\n",
    "\n",
    "        # Residual connection can be added here if desired, e.g., return fused_out + fused\n",
    "        return fused_out\n",
    "\n",
    "class UpscaleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upscaling block using PixelShuffle\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, scale_factor):\n",
    "        super(UpscaleBlock, self).__init__()\n",
    "        # Convolution increases channels to in_channels * (scale_factor^2)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels * (scale_factor ** 2), kernel_size=3, stride=1, padding=1)\n",
    "        # PixelShuffle rearranges elements from [B, C * r^2, H, W] to [B, C, H * r, W * r]\n",
    "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
    "        self.prelu = nn.PReLU(num_parameters=in_channels) # Activation after pixel shuffle\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class HybridFrequencySpatialNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Hybrid Network for frequency-spatial image restoration\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=3, base_channels=64, scale_factor=2):\n",
    "        super(HybridFrequencySpatialNetwork, self).__init__()\n",
    "\n",
    "        # Initial shallow feature extraction from input LR image\n",
    "        self.feature_extract = nn.Conv2d(in_channels, base_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Spatial processing branch\n",
    "        self.spatial_branch = SpatialBranch(in_channels=base_channels, base_channels=base_channels,\n",
    "                                           num_blocks=cfg.num_residual_blocks // 2) # Half blocks here\n",
    "\n",
    "        # Frequency processing branch\n",
    "        self.frequency_branch = FrequencyBranch(in_channels=base_channels, base_channels=base_channels)\n",
    "\n",
    "        # Feature fusion module\n",
    "        self.fusion = AttentionFusion(base_channels)\n",
    "\n",
    "        # Deep feature extraction after fusion using Residual Groups\n",
    "        self.deep_features = ResidualGroup(base_channels, n_blocks=cfg.num_residual_blocks, # All blocks here\n",
    "                                          use_ca=cfg.use_channel_attention,\n",
    "                                          use_sa=cfg.use_spatial_attention)\n",
    "\n",
    "        # Reconstruction module (Upscaling + final convolution)\n",
    "        self.reconstruction = nn.Sequential(\n",
    "            # Optional: Conv before upscaling\n",
    "            # nn.Conv2d(base_channels, base_channels, 3, 1, 1),\n",
    "            UpscaleBlock(base_channels, scale_factor),\n",
    "            # Final convolution to get the output image channels\n",
    "            nn.Conv2d(base_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            # Optional: Add Tanh or Sigmoid activation if output needs to be in a specific range,\n",
    "            # but usually not needed if target is [0, 1] and using L1/MSE loss.\n",
    "            # nn.Tanh() or nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Input LR image tensor [B, 3, H_lr, W_lr]\n",
    "\n",
    "        # Extract initial features\n",
    "        initial_features = F.leaky_relu(self.feature_extract(x), 0.2, inplace=True) # [B, C, H_lr, W_lr]\n",
    "\n",
    "        # Process through spatial branch\n",
    "        spatial_features = self.spatial_branch(initial_features) # [B, C, H_lr, W_lr]\n",
    "\n",
    "        # Process through frequency branch\n",
    "        # freq_features are the spatial-domain representation from mag/phase path\n",
    "        # complex_features are the processed (real, imag) tuple from complex path\n",
    "        freq_features, complex_features = self.frequency_branch(initial_features) # [B, C, H_lr, W_lr], ([B,C,H,W], [B,C,H,W])\n",
    "\n",
    "        # Fuse features from both branches\n",
    "        fused_features = self.fusion(spatial_features, freq_features) # [B, C, H_lr, W_lr]\n",
    "\n",
    "        # Process fused features through deep feature extraction block\n",
    "        deep_features = self.deep_features(fused_features) # [B, C, H_lr, W_lr]\n",
    "\n",
    "        # Reconstruct the high-resolution image\n",
    "        output = self.reconstruction(deep_features) # [B, 3, H_hr, W_hr]\n",
    "\n",
    "        # Return final output and intermediate features (optional, for potential auxiliary losses)\n",
    "        return output, (spatial_features, freq_features, complex_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e21f7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function comparing magnitude and phase in the frequency domain\n",
    "    \"\"\"\n",
    "    def __init__(self, use_log_magnitude=True, phase_weight=0.5):\n",
    "        super(FrequencyLoss, self).__init__()\n",
    "        self.use_log_magnitude = use_log_magnitude\n",
    "        self.phase_weight = phase_weight\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # Ensure input tensors are floating point\n",
    "        output = output.float()\n",
    "        target = target.float()\n",
    "\n",
    "        # Convert to frequency domain using FFT\n",
    "        output_fft = torch.fft.fft2(output, norm='ortho')\n",
    "        target_fft = torch.fft.fft2(target, norm='ortho')\n",
    "\n",
    "        # Compute magnitude difference\n",
    "        output_magnitude = torch.abs(output_fft) # Magnitude = sqrt(real^2 + imag^2)\n",
    "        target_magnitude = torch.abs(target_fft)\n",
    "\n",
    "        if self.use_log_magnitude:\n",
    "            # Use L1 loss on log magnitude for better handling of dynamic range\n",
    "            output_log_magnitude = torch.log(output_magnitude + 1e-8) # Add epsilon before log\n",
    "            target_log_magnitude = torch.log(target_magnitude + 1e-8)\n",
    "            magnitude_loss = self.l1_loss(output_log_magnitude, target_log_magnitude)\n",
    "        else:\n",
    "            # Use L1 loss directly on magnitude\n",
    "            magnitude_loss = self.l1_loss(output_magnitude, target_magnitude)\n",
    "\n",
    "        # Compute phase difference\n",
    "        output_phase = torch.angle(output_fft) # Phase = atan2(imag, real)\n",
    "        target_phase = torch.angle(target_fft)\n",
    "\n",
    "        # Phase loss (L1 distance, accounting for phase wrapping [-pi, pi])\n",
    "        phase_diff = output_phase - target_phase\n",
    "        # Map difference to [-pi, pi]\n",
    "        phase_diff = torch.remainder(phase_diff + np.pi, 2 * np.pi) - np.pi\n",
    "        phase_loss = torch.abs(phase_diff).mean() # Mean absolute phase error\n",
    "\n",
    "        # Combined loss\n",
    "        return magnitude_loss + self.phase_weight * phase_loss\n",
    "\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceptual loss using VGG19 features.\n",
    "    Compares feature maps from specific layers of a pre-trained VGG network.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_layers=[2, 7, 12, 21, 30], use_input_norm=True):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "\n",
    "        # Load pre-trained VGG19 model\n",
    "        vgg = vgg19(weights=VGG19_Weights.DEFAULT).features.eval() # Set to evaluation mode\n",
    "\n",
    "        # Normalize input based on ImageNet mean/std if needed\n",
    "        self.use_input_norm = use_input_norm\n",
    "        if self.use_input_norm:\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "            self.register_buffer('mean', mean)\n",
    "            self.register_buffer('std', std)\n",
    "\n",
    "        self.feature_extractors = nn.ModuleList()\n",
    "        self.feature_layers = feature_layers # Layers to extract features from\n",
    "\n",
    "        # Create feature extractors up to the maximum layer needed\n",
    "        for i in range(max(feature_layers) + 1):\n",
    "            layer = vgg[i]\n",
    "            # Freeze VGG parameters\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.feature_extractors.append(layer)\n",
    "\n",
    "        # Use L1 loss for comparing feature maps\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        # Ensure inputs are on the same device as the model\n",
    "        x = x.to(next(self.parameters()).device)\n",
    "        target = target.to(next(self.parameters()).device)\n",
    "\n",
    "        # Normalize inputs if required\n",
    "        if self.use_input_norm:\n",
    "            x = (x - self.mean) / self.std\n",
    "            target = (target - self.mean) / self.std\n",
    "\n",
    "        perceptual_loss = 0.0\n",
    "        current_x = x\n",
    "        current_target = target\n",
    "\n",
    "        # Extract features layer by layer and compute loss\n",
    "        for i, layer in enumerate(self.feature_extractors):\n",
    "            current_x = layer(current_x)\n",
    "            current_target = layer(current_target)\n",
    "\n",
    "            if i in self.feature_layers:\n",
    "                perceptual_loss += self.loss_fn(current_x, current_target)\n",
    "\n",
    "        return perceptual_loss / len(self.feature_layers) # Average loss over selected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a3c9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions ---\n",
    "\n",
    "def calculate_psnr(img1, img2, data_range=1.0):\n",
    "    \"\"\"Calculate Peak Signal-to-Noise Ratio (PSNR) between two images.\"\"\"\n",
    "    # Ensure images are numpy arrays and float type\n",
    "    img1 = np.asarray(img1, dtype=np.float32)\n",
    "    img2 = np.asarray(img2, dtype=np.float32)\n",
    "\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf') # PSNR is infinite if images are identical\n",
    "    psnr = 20 * math.log10(data_range / math.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "def calculate_ssim(img1, img2, data_range=1.0, win_size=7):\n",
    "    \"\"\"\n",
    "    Calculate Structural Similarity Index (SSIM) between two images.\n",
    "    Handles cases where images are too small for the specified window size.\n",
    "    \"\"\"\n",
    "    # Ensure images are numpy arrays and float type\n",
    "    img1 = np.asarray(img1, dtype=np.float32)\n",
    "    img2 = np.asarray(img2, dtype=np.float32)\n",
    "\n",
    "    # SSIM expects channel dimension last or grayscale\n",
    "    if img1.ndim == 3 and img1.shape[0] in [1, 3]: # If channel is first (e.g., [C, H, W])\n",
    "        img1 = img1.transpose(1, 2, 0) # Convert to [H, W, C]\n",
    "    if img2.ndim == 3 and img2.shape[0] in [1, 3]:\n",
    "        img2 = img2.transpose(1, 2, 0)\n",
    "\n",
    "    # --- FIX: Check image dimensions before calculating SSIM ---\n",
    "    # Get spatial dimensions (height, width)\n",
    "    if img1.ndim == 3: # HWC\n",
    "        h1, w1, _ = img1.shape\n",
    "        h2, w2, _ = img2.shape\n",
    "    elif img1.ndim == 2: # HW (grayscale)\n",
    "        h1, w1 = img1.shape\n",
    "        h2, w2 = img2.shape\n",
    "    else:\n",
    "        warnings.warn(f\"Unexpected image dimension: {img1.ndim}. Skipping SSIM calculation.\")\n",
    "        return 0.0 # Cannot calculate SSIM\n",
    "\n",
    "    # Check if window size is valid for both images\n",
    "    if h1 < win_size or w1 < win_size or h2 < win_size or w2 < win_size:\n",
    "        warnings.warn(\n",
    "            f\"Image size ({h1}x{w1}, {h2}x{w2}) is too small for \"\n",
    "            f\"SSIM calculation with win_size={win_size}. Returning SSIM=0.0.\"\n",
    "        )\n",
    "        return 0.0 # Return 0.0 if images are too small\n",
    "\n",
    "    # Ensure win_size is odd\n",
    "    if win_size % 2 == 0:\n",
    "        warnings.warn(f\"win_size must be odd, but got {win_size}. Using win_size={win_size+1}\")\n",
    "        win_size += 1\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    # Calculate SSIM using skimage\n",
    "    # For multichannel images (RGB), set channel_axis=-1 (last axis)\n",
    "    is_multichannel = img1.ndim == 3 and img1.shape[-1] == 3\n",
    "    channel_axis = -1 if is_multichannel else None\n",
    "\n",
    "    # Use data_range argument correctly\n",
    "    ssim_val = ssim_metric(\n",
    "        img1, img2,\n",
    "        win_size=win_size,\n",
    "        data_range=data_range,\n",
    "        channel_axis=channel_axis # Use channel_axis instead of multichannel\n",
    "    )\n",
    "    return ssim_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "853cad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, val_loader, device, epoch, save_dir):\n",
    "    \"\"\"Visualize model results on a validation sample and save the plot.\"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        try:\n",
    "            # Get a batch of validation images\n",
    "            batch = next(iter(val_loader))\n",
    "            lr_imgs = batch['lr'].to(device) # Shape [B, C, H_lr, W_lr]\n",
    "            hr_imgs = batch['hr'].to(device) # Shape [B, C, H_hr, W_hr]\n",
    "\n",
    "            # Generate predictions using the model\n",
    "            outputs, _ = model(lr_imgs) # Shape [B, C, H_hr, W_hr]\n",
    "\n",
    "            # Select the first image from the batch for visualization\n",
    "            lr_img = lr_imgs[0].cpu().numpy().transpose(1, 2, 0) # Convert to HWC, numpy\n",
    "            sr_img = outputs[0].cpu().numpy().transpose(1, 2, 0) # Convert to HWC, numpy\n",
    "            hr_img = hr_imgs[0].cpu().numpy().transpose(1, 2, 0) # Convert to HWC, numpy\n",
    "\n",
    "            # Clip predictions to the valid range [0, 1]\n",
    "            sr_img = np.clip(sr_img, 0, 1)\n",
    "            hr_img = np.clip(hr_img, 0, 1) # Also clip HR just in case\n",
    "            lr_img = np.clip(lr_img, 0, 1) # Also clip LR\n",
    "\n",
    "            # Calculate metrics for the visualized sample\n",
    "            psnr = calculate_psnr(sr_img, hr_img, data_range=1.0)\n",
    "            ssim = calculate_ssim(sr_img, hr_img, data_range=1.0)\n",
    "\n",
    "            # Plot results: LR, SR (Super-Resolved), HR (High-Resolution)\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 6)) # Create figure and axes\n",
    "\n",
    "            # Plot Low Resolution\n",
    "            axes[0].imshow(lr_img, vmin=0, vmax=1)\n",
    "            axes[0].set_title('Low Resolution Input')\n",
    "            axes[0].axis('off')\n",
    "\n",
    "            # Plot Super Resolution\n",
    "            axes[1].imshow(sr_img, vmin=0, vmax=1)\n",
    "            axes[1].set_title(f'Super Resolution Output\\nPSNR: {psnr:.2f} dB, SSIM: {ssim:.4f}')\n",
    "            axes[1].axis('off')\n",
    "\n",
    "            # Plot High Resolution Ground Truth\n",
    "            axes[2].imshow(hr_img, vmin=0, vmax=1)\n",
    "            axes[2].set_title('High Resolution Ground Truth')\n",
    "            axes[2].axis('off')\n",
    "\n",
    "            plt.tight_layout() # Adjust layout\n",
    "            save_path = os.path.join(save_dir, f'results_epoch_{epoch:03d}.png')\n",
    "            plt.savefig(save_path) # Save the figure\n",
    "            print(f\"Saved visualization to {save_path}\")\n",
    "            plt.close(fig) # Close the figure to free memory\n",
    "\n",
    "            # Log to wandb if enabled\n",
    "            if cfg.use_wandb:\n",
    "                wandb.log({\n",
    "                    f\"examples/epoch_{epoch}\": wandb.Image(save_path),\n",
    "                    'epoch': epoch # Ensure epoch is logged for x-axis\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during visualization: {e}\")\n",
    "            # If val_loader is exhausted, reset it or handle appropriately\n",
    "            if isinstance(e, StopIteration):\n",
    "                print(\"Validation loader exhausted during visualization.\")\n",
    "            # Optionally, re-raise the error or continue training\n",
    "            # raise e\n",
    "\n",
    "\n",
    "def plot_training_curves(train_losses, val_psnrs, save_dir):\n",
    "    \"\"\"Plot and save training loss and validation PSNR curves.\"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Training Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Validation PSNR\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, val_psnrs, label='Validation PSNR', color='orange')\n",
    "    plt.title('Validation PSNR per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('PSNR (dB)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'training_curves.png')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Saved training curves to {save_path}\")\n",
    "    plt.close() # Close the plot\n",
    "\n",
    "    # Log curves to wandb if enabled\n",
    "    if cfg.use_wandb:\n",
    "        for i in range(len(train_losses)):\n",
    "             wandb.log({'chart/train_loss': train_losses[i], 'chart/val_psnr': val_psnrs[i], 'epoch': i + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d23da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, val_psnr, save_path):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_psnr': val_psnr,\n",
    "        'config': vars(cfg) # Save config for reference\n",
    "    }\n",
    "    torch.save(state, save_path)\n",
    "    print(f\"Checkpoint saved to {save_path} at epoch {epoch}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path, device):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "        return 0, 0.0 # Return start epoch 0, best PSNR 0\n",
    "\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Load model state\n",
    "    # Handle potential DataParallel prefix if model was saved with it\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if list(state_dict.keys())[0].startswith('module.'):\n",
    "        state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Load optimizer and scheduler state\n",
    "    if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    start_epoch = checkpoint.get('epoch', -1) + 1 # Resume from next epoch\n",
    "    best_val_psnr = checkpoint.get('val_psnr', 0.0)\n",
    "\n",
    "    print(f\"Resuming training from epoch {start_epoch} with best validation PSNR: {best_val_psnr:.4f}\")\n",
    "    return start_epoch, best_val_psnr\n",
    "\n",
    "\n",
    "def warmup_learning_rate(base_lr, current_epoch, warmup_epochs):\n",
    "    \"\"\"Linear warm-up for learning rate.\"\"\"\n",
    "    if current_epoch >= warmup_epochs:\n",
    "        return base_lr # Return base LR after warmup\n",
    "    # Linearly increase LR from a small value (e.g., base_lr / 10) to base_lr\n",
    "    start_lr = base_lr / 10\n",
    "    return start_lr + (base_lr - start_lr) * (current_epoch / warmup_epochs)\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Train the hybrid frequency-spatial image restoration model.\"\"\"\n",
    "    # --- Setup ---\n",
    "    # Create data loaders\n",
    "    try:\n",
    "        train_dataset = DIV2KDataset(\n",
    "            cfg.data_root,\n",
    "            split='train',\n",
    "            scale=cfg.scale,\n",
    "            patch_size=cfg.patch_size,\n",
    "            augment=True\n",
    "        )\n",
    "        val_dataset = DIV2KDataset(\n",
    "            cfg.data_root,\n",
    "            split='valid',\n",
    "            scale=cfg.scale,\n",
    "            patch_size=cfg.patch_size, # Use full images or large patches for validation? Usually full images.\n",
    "            augment=False\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error creating dataset: {e}\")\n",
    "        print(\"Please ensure the DIV2K dataset is correctly placed and the 'data_root' path is correct.\")\n",
    "        return None, [], [] # Exit gracefully\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True, # Improves data transfer speed to GPU\n",
    "        drop_last=True # Drop last incomplete batch\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1, # Validate one image at a time for metric calculation\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    model = HybridFrequencySpatialNetwork(\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        base_channels=cfg.base_channels,\n",
    "        scale_factor=cfg.scale\n",
    "    )\n",
    "\n",
    "    # Move model to device (GPU or CPU)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optional: Use DataParallel for multi-GPU training\n",
    "    # if torch.cuda.device_count() > 1:\n",
    "    #    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    #    model = nn.DataParallel(model)\n",
    "\n",
    "    # Define loss functions\n",
    "    l1_loss = nn.L1Loss().to(device)\n",
    "    perceptual_loss = PerceptualLoss().to(device)\n",
    "    frequency_loss = FrequencyLoss().to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=cfg.lr,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        betas=(0.9, 0.999) # Default Adam betas\n",
    "    )\n",
    "\n",
    "    # Define learning rate scheduler\n",
    "    # Cosine Annealing with Warm Restarts\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=cfg.num_epochs // 10 if cfg.num_epochs >= 10 else cfg.num_epochs, # Number of epochs for the first restart\n",
    "        T_mult=2, # Factor to increase T_i after a restart\n",
    "        eta_min=cfg.min_lr # Minimum learning rate\n",
    "    )\n",
    "\n",
    "    # Gradient scaler for Automatic Mixed Precision (AMP)\n",
    "    scaler = GradScaler(enabled=cfg.use_amp)\n",
    "\n",
    "    # Initialize statistics\n",
    "    best_val_psnr = 0.0\n",
    "    train_losses = []\n",
    "    val_psnrs = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    # --- Checkpoint Loading ---\n",
    "    best_checkpoint_path = os.path.join(cfg.save_dir, 'best_model.pth')\n",
    "    latest_checkpoint_path = os.path.join(cfg.save_dir, 'latest_model.pth') # Optional: save latest checkpoint too\n",
    "\n",
    "    # Prefer loading the best model if it exists\n",
    "    load_path = best_checkpoint_path if os.path.exists(best_checkpoint_path) else latest_checkpoint_path\n",
    "    if os.path.exists(load_path):\n",
    "         start_epoch, best_val_psnr = load_checkpoint(model, optimizer, scheduler, load_path, device)\n",
    "         # Reload stats if needed (e.g., load train_losses, val_psnrs from checkpoint)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    print(f\"Starting training from epoch {start_epoch}...\")\n",
    "    for epoch in range(start_epoch, cfg.num_epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{cfg.num_epochs} ---\")\n",
    "        model.train() # Set model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        epoch_l1_loss = 0.0\n",
    "        epoch_perc_loss = 0.0\n",
    "        epoch_freq_loss = 0.0\n",
    "\n",
    "        # Initialize progress bar for the training epoch\n",
    "        pbar = tqdm(train_loader, desc=f\"Train E{epoch+1}\", leave=False)\n",
    "\n",
    "        # Apply learning rate warm-up if applicable\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # Get current LR before potential warmup adjustment\n",
    "        if epoch < cfg.warmup_epochs:\n",
    "            new_lr = warmup_learning_rate(cfg.lr, epoch, cfg.warmup_epochs)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "            current_lr = new_lr # Update current_lr for logging\n",
    "            print(f\"Warm-up Epoch {epoch+1}: LR set to {current_lr:.6f}\")\n",
    "        else:\n",
    "             print(f\"Epoch {epoch+1}: LR = {current_lr:.6f}\")\n",
    "\n",
    "\n",
    "        # --- Training Batch Loop ---\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Get data and move to device\n",
    "            lr_imgs = batch['lr'].to(device, non_blocking=True) # Use non_blocking for potential speedup\n",
    "            hr_imgs = batch['hr'].to(device, non_blocking=True)\n",
    "\n",
    "            # Zero gradients before the backward pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with Automatic Mixed Precision (AMP) context\n",
    "            with autocast(enabled=cfg.use_amp):\n",
    "                # Get model outputs\n",
    "                outputs, (spatial_features, freq_features, complex_features) = model(lr_imgs)\n",
    "                # Ensure outputs are float32 for loss calculation if needed\n",
    "                outputs = outputs.float()\n",
    "\n",
    "                # Calculate individual loss components\n",
    "                loss_l1 = l1_loss(outputs, hr_imgs)\n",
    "                loss_perceptual = perceptual_loss(outputs, hr_imgs)\n",
    "                loss_frequency = frequency_loss(outputs, hr_imgs)\n",
    "\n",
    "                # Combine losses with specified weights\n",
    "                total_loss = (cfg.l1_weight * loss_l1 +\n",
    "                              cfg.perceptual_weight * loss_perceptual +\n",
    "                              cfg.freq_loss_weight * loss_frequency)\n",
    "\n",
    "            # Backward pass: Calculate gradients using the scaled loss\n",
    "            scaler.scale(total_loss).backward()\n",
    "\n",
    "            # Optional: Gradient clipping (unscale first)\n",
    "            scaler.unscale_(optimizer) # Unscales gradients inplace\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Clip gradient norm\n",
    "\n",
    "            # Optimizer step: Update model weights using scaled gradients\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            # Update the scale factor for the next iteration\n",
    "            scaler.update()\n",
    "\n",
    "            # --- Logging and Progress ---\n",
    "            batch_loss = total_loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "            epoch_l1_loss += loss_l1.item()\n",
    "            epoch_perc_loss += loss_perceptual.item()\n",
    "            epoch_freq_loss += loss_frequency.item()\n",
    "\n",
    "            # Update progress bar description\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{batch_loss:.4f}',\n",
    "                'L1': f'{loss_l1.item():.4f}',\n",
    "                'Perc': f'{loss_perceptual.item():.4f}',\n",
    "                'Freq': f'{loss_frequency.item():.4f}',\n",
    "                'LR': f'{current_lr:.1e}'\n",
    "            })\n",
    "\n",
    "            # Log batch metrics to wandb if enabled\n",
    "            if cfg.use_wandb and batch_idx % cfg.log_interval == 0:\n",
    "                step = epoch * len(train_loader) + batch_idx\n",
    "                wandb.log({\n",
    "                    'train/batch_loss': batch_loss,\n",
    "                    'train/batch_l1_loss': loss_l1.item(),\n",
    "                    'train/batch_perceptual_loss': loss_perceptual.item(),\n",
    "                    'train/batch_frequency_loss': loss_frequency.item(),\n",
    "                    'train/learning_rate': current_lr,\n",
    "                    'step': step,\n",
    "                    'epoch': epoch + (batch_idx / len(train_loader)) # Log fractional epoch\n",
    "                })\n",
    "\n",
    "        # --- End of Epoch ---\n",
    "        # Calculate average losses for the epoch\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        avg_l1_loss = epoch_l1_loss / len(train_loader)\n",
    "        avg_perc_loss = epoch_perc_loss / len(train_loader)\n",
    "        avg_freq_loss = epoch_freq_loss / len(train_loader)\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Average Train Loss: {avg_epoch_loss:.4f} \"\n",
    "              f\"(L1: {avg_l1_loss:.4f}, Perc: {avg_perc_loss:.4f}, Freq: {avg_freq_loss:.4f})\")\n",
    "\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        total_val_psnr = 0.0\n",
    "        total_val_ssim = 0.0\n",
    "\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Validate E{epoch+1}\", leave=False)\n",
    "        with torch.no_grad(): # Disable gradient calculations during validation\n",
    "            for batch in val_pbar:\n",
    "                lr_imgs = batch['lr'].to(device, non_blocking=True)\n",
    "                hr_imgs = batch['hr'].to(device, non_blocking=True)\n",
    "\n",
    "                # Forward pass (no need for AMP context here, but doesn't hurt)\n",
    "                with autocast(enabled=cfg.use_amp):\n",
    "                    outputs, _ = model(lr_imgs)\n",
    "                    outputs = outputs.float() # Ensure float32 for metrics\n",
    "\n",
    "                # Convert tensors to numpy arrays for metric calculation\n",
    "                # Process each image in the batch (batch size is 1 here)\n",
    "                output_np = outputs[0].cpu().numpy().transpose(1, 2, 0) # HWC format\n",
    "                target_np = hr_imgs[0].cpu().numpy().transpose(1, 2, 0) # HWC format\n",
    "\n",
    "                # Clip predictions to the valid range [0, 1] before calculating metrics\n",
    "                output_np = np.clip(output_np, 0, 1)\n",
    "                target_np = np.clip(target_np, 0, 1)\n",
    "\n",
    "                # Calculate metrics\n",
    "                psnr = calculate_psnr(output_np, target_np, data_range=1.0)\n",
    "                ssim = calculate_ssim(output_np, target_np, data_range=1.0)\n",
    "\n",
    "                total_val_psnr += psnr\n",
    "                total_val_ssim += ssim\n",
    "                val_pbar.set_postfix({'PSNR': f'{psnr:.2f}', 'SSIM': f'{ssim:.4f}'})\n",
    "\n",
    "\n",
    "        # Calculate average validation metrics\n",
    "        avg_val_psnr = total_val_psnr / len(val_loader)\n",
    "        avg_val_ssim = total_val_ssim / len(val_loader)\n",
    "        val_psnrs.append(avg_val_psnr)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Validation Results - Avg PSNR: {avg_val_psnr:.4f} dB, Avg SSIM: {avg_val_ssim:.4f}\")\n",
    "\n",
    "        # Update learning rate scheduler (step after validation)\n",
    "        # Note: Step scheduler based on epoch, not validation score unless it's ReduceLROnPlateau\n",
    "        if epoch >= cfg.warmup_epochs: # Don't step scheduler during warmup\n",
    "             scheduler.step()\n",
    "\n",
    "\n",
    "        # --- Logging Epoch Results ---\n",
    "        if cfg.use_wandb:\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1, # Log integer epoch\n",
    "                'train/epoch_loss': avg_epoch_loss,\n",
    "                'train/epoch_l1_loss': avg_l1_loss,\n",
    "                'train/epoch_perceptual_loss': avg_perc_loss,\n",
    "                'train/epoch_frequency_loss': avg_freq_loss,\n",
    "                'val/avg_psnr': avg_val_psnr,\n",
    "                'val/avg_ssim': avg_val_ssim,\n",
    "                'train/learning_rate': current_lr # Log LR used for the epoch\n",
    "            })\n",
    "\n",
    "        # --- Checkpointing ---\n",
    "        # Save the latest model state\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, epoch, avg_val_psnr,\n",
    "            latest_checkpoint_path\n",
    "        )\n",
    "\n",
    "        # Save checkpoint if validation PSNR improved\n",
    "        if avg_val_psnr > best_val_psnr:\n",
    "            best_val_psnr = avg_val_psnr\n",
    "            save_checkpoint(\n",
    "                model, optimizer, scheduler, epoch, best_val_psnr,\n",
    "                best_checkpoint_path\n",
    "            )\n",
    "            print(f\"*** New best model saved with PSNR: {best_val_psnr:.4f} ***\")\n",
    "\n",
    "        # Save periodic checkpoint (optional)\n",
    "        if (epoch + 1) % cfg.checkpoint_interval == 0:\n",
    "            periodic_path = os.path.join(cfg.save_dir, f'checkpoint_epoch_{epoch+1:03d}.pth')\n",
    "            save_checkpoint(\n",
    "                model, optimizer, scheduler, epoch, avg_val_psnr,\n",
    "                periodic_path\n",
    "            )\n",
    "\n",
    "        # --- Visualization ---\n",
    "        # Visualize some results periodically (e.g., every 10 epochs)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == cfg.num_epochs - 1:\n",
    "            visualize_results(model, val_loader, device, epoch + 1, cfg.save_dir)\n",
    "\n",
    "\n",
    "        # --- Early Stopping (Optional) ---\n",
    "        # Add early stopping logic if desired, e.g., stop if val PSNR doesn't improve for N epochs.\n",
    "        patience = 15\n",
    "        if len(val_psnrs) > patience and avg_val_psnr < max(val_psnrs[-patience-1:-1]):\n",
    "             print(f\"Validation PSNR did not improve for {patience} epochs. Early stopping.\")\n",
    "             break\n",
    "\n",
    "\n",
    "    # --- End of Training ---\n",
    "    print(\"\\nTraining finished!\")\n",
    "    print(f\"Best validation PSNR achieved: {best_val_psnr:.4f}\")\n",
    "\n",
    "    # Plot final training curves\n",
    "    plot_training_curves(train_losses, val_psnrs, cfg.save_dir)\n",
    "\n",
    "    # Cleanup wandb run\n",
    "    if cfg.use_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "    return model, train_losses, val_psnrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58f4a188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hybrid Frequency-Spatial Image Restoration Training...\n",
      "Configuration: {}\n",
      "Loaded 800 images for train\n",
      "Loaded 100 images for valid\n",
      "Loading checkpoint from results\\best_model.pth...\n",
      "Resuming training from epoch 1 with best validation PSNR: 12.2912\n",
      "Starting training from epoch 1...\n",
      "\n",
      "--- Epoch 2/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E2:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 2: LR set to 0.000056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Average Train Loss: 0.7110 (L1: 0.1842, Perc: 4.1059, Freq: 2.3238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Results - Avg PSNR: 13.4346 dB, Avg SSIM: 0.3652\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 1\n",
      "Checkpoint saved to results\\best_model.pth at epoch 1\n",
      "*** New best model saved with PSNR: 13.4346 ***\n",
      "\n",
      "--- Epoch 3/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E3:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 3: LR set to 0.000092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Average Train Loss: 0.5780 (L1: 0.1224, Perc: 3.5627, Freq: 1.9865)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Results - Avg PSNR: 14.8454 dB, Avg SSIM: 0.4698\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 2\n",
      "Checkpoint saved to results\\best_model.pth at epoch 2\n",
      "*** New best model saved with PSNR: 14.8454 ***\n",
      "\n",
      "--- Epoch 4/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E4:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 4: LR set to 0.000128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Average Train Loss: 0.5186 (L1: 0.1053, Perc: 3.1984, Freq: 1.8692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Results - Avg PSNR: 12.3651 dB, Avg SSIM: 0.4354\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 3\n",
      "\n",
      "--- Epoch 5/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E5:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up Epoch 5: LR set to 0.000164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Average Train Loss: 0.4998 (L1: 0.0966, Perc: 3.1409, Freq: 1.7832)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Results - Avg PSNR: 13.4819 dB, Avg SSIM: 0.4841\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 4\n",
      "Checkpoint saved to results\\checkpoint_epoch_005.pth at epoch 4\n",
      "\n",
      "--- Epoch 6/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E6:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: LR = 0.000164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Average Train Loss: 0.4687 (L1: 0.0931, Perc: 2.9001, Freq: 1.7108)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Results - Avg PSNR: 13.5431 dB, Avg SSIM: 0.4809\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 5\n",
      "\n",
      "--- Epoch 7/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E7:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: LR = 0.000195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Average Train Loss: 0.4518 (L1: 0.0906, Perc: 2.7754, Freq: 1.6727)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Results - Avg PSNR: 12.3039 dB, Avg SSIM: 0.4582\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 6\n",
      "\n",
      "--- Epoch 8/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E8:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: LR = 0.000181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Average Train Loss: 0.4287 (L1: 0.0845, Perc: 2.6395, Freq: 1.6050)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Results - Avg PSNR: 12.7601 dB, Avg SSIM: 0.4483\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 7\n",
      "\n",
      "--- Epoch 9/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E9:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: LR = 0.000159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Average Train Loss: 0.4159 (L1: 0.0826, Perc: 2.5469, Freq: 1.5729)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Results - Avg PSNR: 13.2515 dB, Avg SSIM: 0.4775\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 8\n",
      "\n",
      "--- Epoch 10/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E10:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: LR = 0.000131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Average Train Loss: 0.4107 (L1: 0.0845, Perc: 2.4876, Freq: 1.5491)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Results - Avg PSNR: 12.7946 dB, Avg SSIM: 0.4676\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 9\n",
      "Checkpoint saved to results\\checkpoint_epoch_010.pth at epoch 9\n",
      "Saved visualization to results\\results_epoch_010.png\n",
      "\n",
      "--- Epoch 11/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E11:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: LR = 0.000101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Average Train Loss: 0.3913 (L1: 0.0766, Perc: 2.3827, Freq: 1.5288)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Results - Avg PSNR: 12.2890 dB, Avg SSIM: 0.4274\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 10\n",
      "\n",
      "--- Epoch 12/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E12:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: LR = 0.000070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Average Train Loss: 0.3761 (L1: 0.0747, Perc: 2.2583, Freq: 1.5106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Results - Avg PSNR: 12.5736 dB, Avg SSIM: 0.4363\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 11\n",
      "\n",
      "--- Epoch 13/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E13:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: LR = 0.000042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Average Train Loss: 0.3714 (L1: 0.0738, Perc: 2.2363, Freq: 1.4780)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Results - Avg PSNR: 12.1984 dB, Avg SSIM: 0.4129\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 12\n",
      "\n",
      "--- Epoch 14/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E14:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: LR = 0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Average Train Loss: 0.3636 (L1: 0.0741, Perc: 2.1646, Freq: 1.4618)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Results - Avg PSNR: 12.0569 dB, Avg SSIM: 0.4101\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 13\n",
      "\n",
      "--- Epoch 15/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E15:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: LR = 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Average Train Loss: 0.3572 (L1: 0.0717, Perc: 2.1269, Freq: 1.4554)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Validation Results - Avg PSNR: 12.5078 dB, Avg SSIM: 0.4380\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 14\n",
      "Checkpoint saved to results\\checkpoint_epoch_015.pth at epoch 14\n",
      "\n",
      "--- Epoch 16/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E16:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: LR = 0.000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Average Train Loss: 0.3899 (L1: 0.0810, Perc: 2.3486, Freq: 1.4813)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Validation Results - Avg PSNR: 12.5694 dB, Avg SSIM: 0.4471\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 15\n",
      "\n",
      "--- Epoch 17/100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E17:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: LR = 0.000199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Average Train Loss: 0.3847 (L1: 0.0788, Perc: 2.3353, Freq: 1.4477)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Validation Results - Avg PSNR: 12.3911 dB, Avg SSIM: 0.4353\n",
      "Checkpoint saved to results\\latest_model.pth at epoch 16\n",
      "Validation PSNR did not improve for 15 epochs. Early stopping.\n",
      "\n",
      "Training finished!\n",
      "Best validation PSNR achieved: 14.8454\n",
      "Saved training curves to results\\training_curves.png\n",
      "\n",
      "Training completed successfully!\n",
      "Final Best Validation PSNR: 14.8454\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to setup and run training\"\"\"\n",
    "    print(\"Starting Hybrid Frequency-Spatial Image Restoration Training...\")\n",
    "    print(f\"Configuration: {vars(cfg)}\")\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize wandb if enabled (moved inside train_model for better scope)\n",
    "\n",
    "    # Train the model\n",
    "    model, train_losses, val_psnrs = train_model()\n",
    "\n",
    "    if model is not None:\n",
    "        print(\"\\nTraining completed successfully!\")\n",
    "        if val_psnrs:\n",
    "            print(f\"Final Best Validation PSNR: {max(val_psnrs):.4f}\")\n",
    "        else:\n",
    "            print(\"No validation results recorded.\")\n",
    "    else:\n",
    "        print(\"\\nTraining failed or was interrupted.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This block runs when the script is executed directly\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
