{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.models import vgg19\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ComplexConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Complex-valued convolutional layer implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(ComplexConv2d, self).__init__()\n",
    "        self.conv_real = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.conv_imag = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a tuple of (real, imag)\n",
    "        real, imag = x\n",
    "        real_out = self.conv_real(real) - self.conv_imag(imag)\n",
    "        imag_out = self.conv_real(imag) + self.conv_imag(real)\n",
    "        return (real_out, imag_out)\n",
    "\n",
    "class DIV2KDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DIV2K with both LR and HR images\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, split=\"train\", scale=2, patch_size=96, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Directory with DIV2K dataset\n",
    "            split: 'train', 'valid', or 'test'\n",
    "            scale: Downscaling factor (2, 3, or 4)\n",
    "            patch_size: Size of cropped patches for training\n",
    "            transform: Additional transforms to apply\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Define paths\n",
    "        self.hr_dir = os.path.join(root_dir, f\"DIV2K_{split}_HR\")\n",
    "        self.lr_dir = os.path.join(root_dir, f\"DIV2K_{split}_LR_bicubic\", f\"X{scale}\")\n",
    "        \n",
    "        # Get all HR images\n",
    "        self.hr_images = sorted(glob.glob(os.path.join(self.hr_dir, \"*.png\")))\n",
    "        \n",
    "        # Make sure we have files\n",
    "        if len(self.hr_images) == 0:\n",
    "            raise RuntimeError(f\"No images found in {self.hr_dir}\")\n",
    "        \n",
    "        # Basic transforms\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get HR image path\n",
    "        hr_path = self.hr_images[idx]\n",
    "        \n",
    "        # Extract image ID from filename (assuming format like \"0001.png\")\n",
    "        img_id = os.path.basename(hr_path).split('.')[0]\n",
    "        \n",
    "        # Construct LR image path (e.g., \"0001x2.png\")\n",
    "        lr_path = os.path.join(self.lr_dir, f\"{img_id}x{self.scale}.png\")\n",
    "        \n",
    "        # Load images\n",
    "        hr_img = Image.open(hr_path).convert('RGB')\n",
    "        lr_img = Image.open(lr_path).convert('RGB')\n",
    "        \n",
    "        # Random crop for training\n",
    "        if self.split == \"train\":\n",
    "            # Get dimensions\n",
    "            hr_width, hr_height = hr_img.size\n",
    "            lr_width, lr_height = lr_img.size\n",
    "            \n",
    "            # Randomly select patch\n",
    "            hr_patch_size = self.patch_size\n",
    "            lr_patch_size = hr_patch_size // self.scale\n",
    "            \n",
    "            # Ensure we can extract a patch of the desired size\n",
    "            if hr_width < hr_patch_size or hr_height < hr_patch_size:\n",
    "                # If image is smaller than patch_size, resize it\n",
    "                hr_img = transforms.Resize((hr_patch_size, hr_patch_size))(hr_img)\n",
    "                lr_img = transforms.Resize((lr_patch_size, lr_patch_size))(lr_img)\n",
    "            else:\n",
    "                # Random crop\n",
    "                hr_x = np.random.randint(0, hr_width - hr_patch_size + 1)\n",
    "                hr_y = np.random.randint(0, hr_height - hr_patch_size + 1)\n",
    "                \n",
    "                lr_x = hr_x // self.scale\n",
    "                lr_y = hr_y // self.scale\n",
    "                \n",
    "                hr_img = hr_img.crop((hr_x, hr_y, hr_x + hr_patch_size, hr_y + hr_patch_size))\n",
    "                lr_img = lr_img.crop((lr_x, lr_y, lr_x + lr_patch_size, lr_y + lr_patch_size))\n",
    "        \n",
    "        # Apply transforms\n",
    "        hr_tensor = self.to_tensor(hr_img)\n",
    "        lr_tensor = self.to_tensor(lr_img)\n",
    "        \n",
    "        if self.transform:\n",
    "            hr_tensor = self.transform(hr_tensor)\n",
    "            lr_tensor = self.transform(lr_tensor)\n",
    "        \n",
    "        return {'lr': lr_tensor, 'hr': hr_tensor}\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic convolutional block with batch normalization and ReLU activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with two convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvBlock(channels, channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        return self.relu(out)\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Downsampling block for U-Net style architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.conv(x)\n",
    "        out = self.pool(features)\n",
    "        return out, features\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block for U-Net style architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class SpatialBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatial branch using a U-Net like architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, base_channels=64):\n",
    "        super(SpatialBranch, self).__init__()\n",
    "        self.init_conv = ConvBlock(in_channels, base_channels)\n",
    "        \n",
    "        # Encoder\n",
    "        self.down1 = DownBlock(base_channels, base_channels*2)\n",
    "        self.down2 = DownBlock(base_channels*2, base_channels*4)\n",
    "        self.down3 = DownBlock(base_channels*4, base_channels*8)\n",
    "        \n",
    "        # Bridge\n",
    "        self.bridge = ConvBlock(base_channels*8, base_channels*16)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = UpBlock(base_channels*16, base_channels*8)\n",
    "        self.up2 = UpBlock(base_channels*8, base_channels*4)\n",
    "        self.up3 = UpBlock(base_channels*4, base_channels*2)\n",
    "        \n",
    "        # Final layers\n",
    "        self.final_conv = ConvBlock(base_channels*2, base_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial features\n",
    "        x1 = self.init_conv(x)\n",
    "        \n",
    "        # Encoder\n",
    "        x2, skip1 = self.down1(x1)\n",
    "        x3, skip2 = self.down2(x2)\n",
    "        x4, skip3 = self.down3(x3)\n",
    "        \n",
    "        # Bridge\n",
    "        x5 = self.bridge(x4)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.up1(x5, skip3)\n",
    "        x = self.up2(x, skip2)\n",
    "        x = self.up3(x, skip1)\n",
    "        \n",
    "        # Final processing\n",
    "        features = self.final_conv(x)\n",
    "        \n",
    "        return features\n",
    "\n",
    "class FrequencyBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Frequency branch that processes the image in frequency domain\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, base_channels=64):\n",
    "        super(FrequencyBranch, self).__init__()\n",
    "        \n",
    "        # Processing for magnitude\n",
    "        self.mag_conv1 = ConvBlock(in_channels, base_channels)\n",
    "        self.mag_res1 = ResidualBlock(base_channels)\n",
    "        self.mag_res2 = ResidualBlock(base_channels)\n",
    "        \n",
    "        # Processing for phase\n",
    "        self.phase_conv1 = ConvBlock(in_channels, base_channels)\n",
    "        self.phase_res1 = ResidualBlock(base_channels)\n",
    "        self.phase_res2 = ResidualBlock(base_channels)\n",
    "        \n",
    "        # Final processing\n",
    "        self.final_conv = ConvBlock(base_channels*2, base_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply FFT to convert to frequency domain\n",
    "        # Need to process each channel separately\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        # Create tensors for storing real and imaginary parts\n",
    "        real_fft = torch.zeros((batch_size, channels, height, width), device=x.device)\n",
    "        imag_fft = torch.zeros((batch_size, channels, height, width), device=x.device)\n",
    "        \n",
    "        # Apply FFT to each channel\n",
    "        for c in range(channels):\n",
    "            fft_result = torch.fft.fft2(x[:, c, :, :])\n",
    "            real_fft[:, c, :, :] = fft_result.real\n",
    "            imag_fft[:, c, :, :] = fft_result.imag\n",
    "        \n",
    "        # Compute magnitude and phase\n",
    "        magnitude = torch.sqrt(real_fft**2 + imag_fft**2)\n",
    "        phase = torch.atan2(imag_fft, real_fft)\n",
    "        \n",
    "        # Apply log scaling to magnitude for better dynamic range\n",
    "        magnitude = torch.log(magnitude + 1e-8)\n",
    "        \n",
    "        # Process magnitude\n",
    "        mag_features = self.mag_conv1(magnitude)\n",
    "        mag_features = self.mag_res1(mag_features)\n",
    "        mag_features = self.mag_res2(mag_features)\n",
    "        \n",
    "        # Process phase\n",
    "        phase_features = self.phase_conv1(phase)\n",
    "        phase_features = self.phase_res1(phase_features)\n",
    "        phase_features = self.phase_res2(phase_features)\n",
    "        \n",
    "        # Concatenate features from magnitude and phase\n",
    "        combined = torch.cat([mag_features, phase_features], dim=1)\n",
    "        \n",
    "        # Final processing\n",
    "        features = self.final_conv(combined)\n",
    "        \n",
    "        return features\n",
    "\n",
    "class FusionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses features from spatial and frequency domains\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=64, out_channels=64):\n",
    "        super(FusionModule, self).__init__()\n",
    "        \n",
    "        # Concatenate and process\n",
    "        self.fusion_conv = ConvBlock(in_channels*2, in_channels)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Final processing\n",
    "        self.final_conv = ConvBlock(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, spatial_features, freq_features):\n",
    "        # Concatenate features\n",
    "        concat_features = torch.cat([spatial_features, freq_features], dim=1)\n",
    "        \n",
    "        # Initial fusion\n",
    "        fused = self.fusion_conv(concat_features)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(fused)\n",
    "        \n",
    "        # Apply attention\n",
    "        fused = fused * attention_weights\n",
    "        \n",
    "        # Final processing\n",
    "        out = self.final_conv(fused)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ReconstructionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Reconstructs the high-resolution image from fused features\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=64, out_channels=3, scale_factor=2):\n",
    "        super(ReconstructionModule, self).__init__()\n",
    "        \n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "        # Upsampling layers\n",
    "        self.upconv1 = nn.Conv2d(in_channels, in_channels*4, kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle1 = nn.PixelShuffle(2)\n",
    "        self.upconv_relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Add another upsampling layer if scale factor is 4\n",
    "        if scale_factor == 4:\n",
    "            self.upconv2 = nn.Conv2d(in_channels, in_channels*4, kernel_size=3, padding=1)\n",
    "            self.pixel_shuffle2 = nn.PixelShuffle(2)\n",
    "            self.upconv_relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Final reconstruction\n",
    "        self.final_conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First upsampling\n",
    "        x = self.upconv1(x)\n",
    "        x = self.pixel_shuffle1(x)\n",
    "        x = self.upconv_relu1(x)\n",
    "        \n",
    "        # Second upsampling if scale factor is 4\n",
    "        if self.scale_factor == 4:\n",
    "            x = self.upconv2(x)\n",
    "            x = self.pixel_shuffle2(x)\n",
    "            x = self.upconv_relu2(x)\n",
    "        elif self.scale_factor == 3:\n",
    "            # For scale factor 3, we need to use interpolation\n",
    "            x = F.interpolate(x, scale_factor=1.5, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Final reconstruction\n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class HybridFrequencySpatialNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete network combining spatial and frequency branches\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=3, base_channels=64, scale_factor=2):\n",
    "        super(HybridFrequencySpatialNetwork, self).__init__()\n",
    "        \n",
    "        self.spatial_branch = SpatialBranch(in_channels, base_channels)\n",
    "        self.frequency_branch = FrequencyBranch(in_channels, base_channels)\n",
    "        self.fusion_module = FusionModule(base_channels, base_channels)\n",
    "        self.reconstruction_module = ReconstructionModule(base_channels, out_channels, scale_factor)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features from both branches\n",
    "        spatial_features = self.spatial_branch(x)\n",
    "        freq_features = self.frequency_branch(x)\n",
    "        \n",
    "        # Fuse features\n",
    "        fused_features = self.fusion_module(spatial_features, freq_features)\n",
    "        \n",
    "        # Reconstruct high-resolution image\n",
    "        output = self.reconstruction_module(fused_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceptual loss using VGG19 features\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_layer=35):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg = vgg19(pretrained=True).features\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg.children())[:feature_layer]).eval()\n",
    "        \n",
    "        # Freeze VGG parameters\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x, target):\n",
    "        # Extract features\n",
    "        x_features = self.feature_extractor(x)\n",
    "        target_features = self.feature_extractor(target)\n",
    "        \n",
    "        # Calculate MSE loss between features\n",
    "        loss = F.mse_loss(x_features, target_features)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"\n",
    "    Calculate PSNR between two images\n",
    "    \"\"\"\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * math.log10(max_pixel / math.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    \"\"\"\n",
    "    Calculate SSIM between two images\n",
    "    Basic implementation - for production use a library like scikit-image\n",
    "    \"\"\"\n",
    "    C1 = (0.01 * 1) ** 2\n",
    "    C2 = (0.03 * 1) ** 2\n",
    "    \n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "    \n",
    "    kernel = np.ones((11, 11)) / 121\n",
    "    \n",
    "    mu1 = np.zeros_like(img1)\n",
    "    mu2 = np.zeros_like(img2)\n",
    "    sigma1_sq = np.zeros_like(img1)\n",
    "    sigma2_sq = np.zeros_like(img2)\n",
    "    sigma12 = np.zeros_like(img1)\n",
    "    \n",
    "    # For each color channel\n",
    "    for i in range(3):\n",
    "        mu1[:, :, i] = np.convolve(img1[:, :, i].flatten(), kernel, mode='valid').reshape(img1.shape[0]-10, img1.shape[1]-10)\n",
    "        mu2[:, :, i] = np.convolve(img2[:, :, i].flatten(), kernel, mode='valid').reshape(img2.shape[0]-10, img2.shape[1]-10)\n",
    "        sigma1_sq[:, :, i] = np.convolve((img1[:, :, i] - mu1[:, :, i])**2, kernel, mode='valid').reshape(img1.shape[0]-10, img1.shape[1]-10)\n",
    "        sigma2_sq[:, :, i] = np.convolve((img2[:, :, i] - mu2[:, :, i])**2, kernel, mode='valid').reshape(img2.shape[0]-10, img2.shape[1]-10)\n",
    "        sigma12[:, :, i] = np.convolve((img1[:, :, i] - mu1[:, :, i]) * (img2[:, :, i] - mu2[:, :, i]), kernel, mode='valid').reshape(img1.shape[0]-10, img1.shape[1]-10)\n",
    "    \n",
    "    # Formula for SSIM\n",
    "    numerator = (2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)\n",
    "    denominator = (mu1**2 + mu2**2 + C1) * (sigma1_sq + sigma2_sq + C2)\n",
    "    ssim_map = numerator / denominator\n",
    "    \n",
    "    # Return mean SSIM across channels\n",
    "    return np.mean(ssim_map)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    # Initialize best validation PSNR\n",
    "    best_val_psnr = 0.0\n",
    "    train_losses = []\n",
    "    val_psnrs = []\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            lr_imgs = batch['lr'].to(device)\n",
    "            hr_imgs = batch['hr'].to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(lr_imgs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, hr_imgs)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            running_loss += loss.item() * lr_imgs.size(0)\n",
    "        \n",
    "        # Calculate epoch loss\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_psnr = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                lr_imgs = batch['lr'].to(device)\n",
    "                hr_imgs = batch['hr'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(lr_imgs)\n",
    "                \n",
    "                # Calculate PSNR\n",
    "                for i in range(outputs.size(0)):\n",
    "                    # Convert to numpy for PSNR calculation\n",
    "                    output = outputs[i].cpu().numpy().transpose(1, 2, 0)\n",
    "                    target = hr_imgs[i].cpu().numpy().transpose(1, 2, 0)\n",
    "                    \n",
    "                    # Clip predictions to valid range\n",
    "                    output = np.clip(output, 0, 1)\n",
    "                    \n",
    "                    # Calculate PSNR\n",
    "                    psnr = calculate_psnr(output, target)\n",
    "                    val_psnr += psnr\n",
    "        \n",
    "        # Calculate average PSNR\n",
    "        val_psnr /= len(val_loader.dataset)\n",
    "        val_psnrs.append(val_psnr)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}, Val PSNR: {val_psnr:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_psnr > best_val_psnr:\n",
    "            best_val_psnr = val_psnr\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"New best model saved!\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_psnrs)\n",
    "    plt.title('Validation PSNR')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('PSNR (dB)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return model, train_losses, val_psnrs\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    batch_size = 16\n",
    "    lr = 1e-4\n",
    "    num_epochs = 50\n",
    "    scale_factor = 2  # 2, 3, or 4\n",
    "    patch_size = 96  # HR patch size\n",
    "    \n",
    "    # Data root directory\n",
    "    data_root = 'DIV2K'\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = DIV2KDataset(data_root, split='train', scale=scale_factor, patch_size=patch_size)\n",
    "    val_dataset = DIV2KDataset(data_root, split='valid', scale=scale_factor, patch_size=patch_size)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Create model\n",
    "    model = HybridFrequencySpatialNetwork(in_channels=3, out_channels=3, base_channels=64, scale_factor=scale_factor)\n",
    "    \n",
    "    # Define loss function\n",
    "    l1_loss = nn.L1Loss()\n",
    "    perceptual_loss = PerceptualLoss().to(device)\n",
    "    \n",
    "    # Combined loss function\n",
    "    def criterion(outputs, targets):\n",
    "        # L1 loss for pixel-level accuracy\n",
    "        loss_l1 = l1_loss(outputs, targets)\n",
    "        \n",
    "        # Perceptual loss for better visual quality\n",
    "        loss_percep = perceptual_loss(outputs, targets)\n",
    "        \n",
    "        # Combine losses\n",
    "        return loss_l1 + 0.1 * loss_percep\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Define scheduler\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=lr*0.1)\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_psnrs = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "        num_epochs=num_epochs, device=device\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n",
    "    \n",
    "    # Test on validation set\n",
    "    model.eval()\n",
    "    val_psnr = 0.0\n",
    "    val_ssim = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            lr_imgs = batch['lr'].to(device)\n",
    "            hr_imgs = batch['hr'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(lr_imgs)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            output = outputs[0].cpu().numpy().transpose(1, 2, 0)\n",
    "            target = hr_imgs[0].cpu().numpy().transpose(1, 2, 0)\n",
    "            \n",
    "            # Clip predictions to valid range\n",
    "            output = np.clip(output, 0, 1)\n",
    "            \n",
    "            # Calculate PSNR and SSIM\n",
    "            psnr = calculate_psnr(output, target)\n",
    "            ssim = calculate_ssim(output, target)\n",
    "            \n",
    "            val_psnr += psnr\n",
    "            val_ssim += ssim\n",
    "            \n",
    "            # Save some example images\n",
    "            if i < 5:\n",
    "                plt.figure(figsize=(15, 5))\n",
    "                \n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(lr_imgs[0].cpu().numpy().transpose(1, 2, 0))\n",
    "                plt.title('LR Image')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(output)\n",
    "                plt.title(f'SR Image (PSNR: {psnr:.2f}, SSIM: {ssim:.4f})')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(target)\n",
    "                plt.title('HR Image')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'example_{i}.png')\n",
    "                plt.close()\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    val_psnr /= len(val_loader)\n",
    "    val_ssim /= len(val_loader)\n",
    "    \n",
    "    print(f\"Final Validation PSNR: {val_psnr:.4f}, SSIM: {val_ssim:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
