{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim_metric\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3852b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2cc8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Dataset\n",
    "    data_root = 'DIV2K'  # Adjust to your path\n",
    "    scale = 2  # Upscaling factor (2, 3, or 4)\n",
    "    patch_size = 128  # HR patch size for training\n",
    "    aug_probability = 0.5  # Probability of applying augmentations\n",
    "    \n",
    "    # Model\n",
    "    base_channels = 64\n",
    "    use_channel_attention = True\n",
    "    use_spatial_attention = True\n",
    "    num_residual_blocks = 16\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 16\n",
    "    num_workers = 1\n",
    "    lr = 2e-4\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-4\n",
    "    num_epochs = 100\n",
    "    warmup_epochs = 5\n",
    "    \n",
    "    # Loss\n",
    "    l1_weight = 1.0\n",
    "    perceptual_weight = 0.1\n",
    "    freq_loss_weight = 0.05\n",
    "    \n",
    "    # Logging\n",
    "    save_dir = 'results'\n",
    "    checkpoint_interval = 5\n",
    "    log_interval = 100\n",
    "    use_wandb = False  # Set to True if you want to use Weights & Biases\n",
    "    \n",
    "    # Mixed precision\n",
    "    use_amp = True\n",
    "\n",
    "# Initialize config\n",
    "cfg = Config()\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "\n",
    "# Initialize wandb if enabled\n",
    "if cfg.use_wandb:\n",
    "    wandb.init(project=\"hybrid-freq-spatial-image-restoration\", config=vars(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2KDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for DIV2K with both LR and HR images\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, split=\"train\", scale=2, patch_size=128, augment=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Directory with DIV2K dataset\n",
    "            split: 'train' or 'valid'\n",
    "            scale: Downscaling factor (2, 3, or 4)\n",
    "            patch_size: Size of cropped patches for training\n",
    "            augment: Whether to apply data augmentation\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.scale = scale\n",
    "        self.patch_size = patch_size\n",
    "        self.augment = augment and split == \"train\"\n",
    "        \n",
    "        # Define paths\n",
    "        self.hr_dir = os.path.join(root_dir, f\"DIV2K_{split}_HR\")\n",
    "        self.lr_dir = os.path.join(root_dir, f\"DIV2K_{split}_LR_bicubic\", f\"X{scale}\")\n",
    "        \n",
    "        # Get all HR images\n",
    "        self.hr_images = sorted(glob.glob(os.path.join(self.hr_dir, \"*.png\")))\n",
    "        \n",
    "        # Make sure we have files\n",
    "        if len(self.hr_images) == 0:\n",
    "            raise RuntimeError(f\"No images found in {self.hr_dir}\")\n",
    "        \n",
    "        # Basic transforms\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "        print(f\"Loaded {len(self.hr_images)} images for {split}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get HR image path\n",
    "        hr_path = self.hr_images[idx]\n",
    "        \n",
    "        # Extract image ID from filename (assuming format like \"0001.png\")\n",
    "        img_id = os.path.basename(hr_path).split('.')[0]\n",
    "        \n",
    "        # Construct LR image path (e.g., \"0001x2.png\")\n",
    "        lr_path = os.path.join(self.lr_dir, f\"{img_id}x{self.scale}.png\")\n",
    "        \n",
    "        # Load images\n",
    "        hr_img = Image.open(hr_path).convert('RGB')\n",
    "        lr_img = Image.open(lr_path).convert('RGB')\n",
    "        \n",
    "        # Random crop for training\n",
    "        if self.split == \"train\":\n",
    "            # Get dimensions\n",
    "            hr_width, hr_height = hr_img.size\n",
    "            \n",
    "            # Randomly select patch\n",
    "            hr_patch_size = self.patch_size\n",
    "            lr_patch_size = hr_patch_size // self.scale\n",
    "            \n",
    "            # Ensure we can extract a patch of the desired size\n",
    "            if hr_width < hr_patch_size or hr_height < hr_patch_size:\n",
    "                # If image is smaller than patch_size, resize it\n",
    "                hr_img = hr_img.resize((hr_patch_size, hr_patch_size), Image.BICUBIC)\n",
    "                lr_img = lr_img.resize((lr_patch_size, lr_patch_size), Image.BICUBIC)\n",
    "            else:\n",
    "                # Random crop\n",
    "                hr_x = random.randint(0, hr_width - hr_patch_size)\n",
    "                hr_y = random.randint(0, hr_height - hr_patch_size)\n",
    "                \n",
    "                lr_x = hr_x // self.scale\n",
    "                lr_y = hr_y // self.scale\n",
    "                \n",
    "                hr_img = hr_img.crop((hr_x, hr_y, hr_x + hr_patch_size, hr_y + hr_patch_size))\n",
    "                lr_img = lr_img.crop((lr_x, lr_y, lr_x + lr_patch_size, lr_y + lr_patch_size))\n",
    "        \n",
    "        # Apply augmentations for training\n",
    "        if self.augment and random.random() < cfg.aug_probability:\n",
    "            # Random horizontal flip\n",
    "            if random.random() < 0.5:\n",
    "                hr_img = hr_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                lr_img = lr_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                \n",
    "            # Random vertical flip\n",
    "            if random.random() < 0.5:\n",
    "                hr_img = hr_img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                lr_img = lr_img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                \n",
    "            # Random rotation (90, 180, 270 degrees)\n",
    "            rot_factor = random.choice([0, 1, 2, 3])\n",
    "            if rot_factor > 0:\n",
    "                hr_img = hr_img.rotate(90 * rot_factor)\n",
    "                lr_img = lr_img.rotate(90 * rot_factor)\n",
    "        \n",
    "        # Apply transforms\n",
    "        hr_tensor = self.to_tensor(hr_img)\n",
    "        lr_tensor = self.to_tensor(lr_img)\n",
    "        \n",
    "        return {'lr': lr_tensor, 'hr': hr_tensor, 'idx': idx, 'hr_path': hr_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f981b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel Attention Module\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel // reduction, channel, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = self.conv(self.avg_pool(x))\n",
    "        max_out = self.conv(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out) * x\n",
    "\n",
    "# Spatial Attention Module\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        y = torch.cat([avg_out, max_out], dim=1)\n",
    "        y = self.conv(y)\n",
    "        return self.sigmoid(y) * x\n",
    "\n",
    "# Residual Channel Attention Block\n",
    "class RCAB(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, use_ca=True, use_sa=False):\n",
    "        super(RCAB, self).__init__()\n",
    "        self.use_ca = use_ca\n",
    "        self.use_sa = use_sa\n",
    "        \n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        )\n",
    "        \n",
    "        if use_ca:\n",
    "            self.ca = ChannelAttention(channels, reduction)\n",
    "        if use_sa:\n",
    "            self.sa = SpatialAttention()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        if self.use_ca:\n",
    "            res = self.ca(res)\n",
    "        if self.use_sa:\n",
    "            res = self.sa(res)\n",
    "        return res + x\n",
    "\n",
    "# Residual Group\n",
    "class ResidualGroup(nn.Module):\n",
    "    def __init__(self, channels, n_blocks=8, use_ca=True, use_sa=False):\n",
    "        super(ResidualGroup, self).__init__()\n",
    "        \n",
    "        body = []\n",
    "        for _ in range(n_blocks):\n",
    "            body.append(RCAB(channels, use_ca=use_ca, use_sa=use_sa))\n",
    "            \n",
    "        self.body = nn.Sequential(*body)\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res = self.conv(res)\n",
    "        return res + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic convolutional block with PReLU activation and instance normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_norm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
    "        if use_norm:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.PReLU(num_parameters=out_channels))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class SpatialBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatial branch using residual groups for feature extraction\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, base_channels=64, num_blocks=8):\n",
    "        super(SpatialBranch, self).__init__()\n",
    "        \n",
    "        # Initial feature extraction\n",
    "        self.conv_first = nn.Conv2d(in_channels, base_channels, 3, 1, 1)\n",
    "        \n",
    "        # Residual Groups with channel attention\n",
    "        self.residual_groups = nn.ModuleList([\n",
    "            ResidualGroup(base_channels, n_blocks=num_blocks, \n",
    "                         use_ca=cfg.use_channel_attention, \n",
    "                         use_sa=cfg.use_spatial_attention)\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "        \n",
    "        # Final feature processing\n",
    "        self.conv_last = nn.Conv2d(base_channels, base_channels, 3, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial feature extraction\n",
    "        x = self.conv_first(x)\n",
    "        residual = x\n",
    "        \n",
    "        # Pass through residual groups\n",
    "        for rg in self.residual_groups:\n",
    "            x = rg(x)\n",
    "        \n",
    "        # Final processing\n",
    "        x = self.conv_last(x)\n",
    "        \n",
    "        # Global residual connection\n",
    "        return x + residual\n",
    "\n",
    "class ComplexConv2d(nn.Module):\n",
    "    \"\"\"Improved complex-valued convolutional layer\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ComplexConv2d, self).__init__()\n",
    "        self.conv_real = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.conv_imag = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a tuple of (real, imag)\n",
    "        real, imag = x\n",
    "        real_out = self.conv_real(real) - self.conv_imag(imag)\n",
    "        imag_out = self.conv_real(imag) + self.conv_imag(real)\n",
    "        return (real_out, imag_out)\n",
    "\n",
    "class ComplexBatchNorm2d(nn.Module):\n",
    "    \"\"\"Complex batch normalization\"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        super(ComplexBatchNorm2d, self).__init__()\n",
    "        self.bn_real = nn.BatchNorm2d(num_features)\n",
    "        self.bn_imag = nn.BatchNorm2d(num_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        real, imag = x\n",
    "        return (self.bn_real(real), self.bn_imag(imag))\n",
    "\n",
    "class ComplexReLU(nn.Module):\n",
    "    \"\"\"Complex ReLU activation\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ComplexReLU, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        real, imag = x\n",
    "        return (self.relu(real), self.relu(imag))\n",
    "\n",
    "class ComplexConvBlock(nn.Module):\n",
    "    \"\"\"Complex convolutional block with normalization and activation\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ComplexConvBlock, self).__init__()\n",
    "        self.conv = ComplexConv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = ComplexBatchNorm2d(out_channels)\n",
    "        self.relu = ComplexReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1fa7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced frequency branch that processes the image in frequency domain\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, base_channels=64):\n",
    "        super(FrequencyBranch, self).__init__()\n",
    "        \n",
    "        # Processing for real and imaginary components\n",
    "        self.complex_blocks = nn.ModuleList([\n",
    "            ComplexConvBlock(base_channels//2, base_channels//2)\n",
    "            for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "        # Magnitude processing\n",
    "        self.magnitude_conv = nn.Sequential(\n",
    "            ConvBlock(in_channels, base_channels//2),\n",
    "            ResidualGroup(base_channels//2, n_blocks=4, \n",
    "                         use_ca=cfg.use_channel_attention, \n",
    "                         use_sa=False)\n",
    "        )\n",
    "        \n",
    "        # Phase processing\n",
    "        self.phase_conv = nn.Sequential(\n",
    "            ConvBlock(in_channels, base_channels//2),\n",
    "            ResidualGroup(base_channels//2, n_blocks=4, \n",
    "                         use_ca=cfg.use_channel_attention, \n",
    "                         use_sa=False)\n",
    "        )\n",
    "        \n",
    "        # Convert from frequency domain features to spatial domain\n",
    "        self.freq_to_spatial = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels, 3, 1, 1),\n",
    "            nn.InstanceNorm2d(base_channels),\n",
    "            nn.PReLU(num_parameters=base_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        # Apply FFT to convert to frequency domain\n",
    "        x_fft = torch.fft.fft2(x)\n",
    "        \n",
    "        # Split into real and imaginary parts\n",
    "        real = x_fft.real\n",
    "        imag = x_fft.imag\n",
    "        \n",
    "        # Compute magnitude and phase\n",
    "        magnitude = torch.sqrt(real**2 + imag**2 + 1e-10)\n",
    "        phase = torch.atan2(imag, real + 1e-10)\n",
    "        \n",
    "        # Apply log scaling to magnitude for better dynamic range\n",
    "        log_magnitude = torch.log(magnitude + 1.0)\n",
    "        \n",
    "        # Process magnitude\n",
    "        mag_features = self.magnitude_conv(log_magnitude)\n",
    "        \n",
    "        # Process phase\n",
    "        phase_features = self.phase_conv(phase)\n",
    "        \n",
    "        # Process complex components directly\n",
    "        complex_features = (real, imag)\n",
    "        for block in self.complex_blocks:\n",
    "            complex_features = block(complex_features)\n",
    "        \n",
    "        real_features, imag_features = complex_features\n",
    "        \n",
    "        # Combine processed features\n",
    "        combined_features = torch.cat([mag_features, phase_features], dim=1)\n",
    "        \n",
    "        # Convert to spatial domain features\n",
    "        spatial_features = self.freq_to_spatial(combined_features)\n",
    "        \n",
    "        return spatial_features, (real_features, imag_features)\n",
    "\n",
    "class AttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced fusion module using channel and spatial attention\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=64):\n",
    "        super(AttentionFusion, self).__init__()\n",
    "        \n",
    "        # Initial fusion\n",
    "        self.conv1 = nn.Conv2d(channels*2, channels, 1, 1, 0)\n",
    "        \n",
    "        # Channel attention\n",
    "        self.ca = ChannelAttention(channels)\n",
    "        \n",
    "        # Spatial attention\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "        # Final processing\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.norm = nn.InstanceNorm2d(channels)\n",
    "        self.prelu = nn.PReLU(num_parameters=channels)\n",
    "        \n",
    "    def forward(self, spatial_features, freq_features):\n",
    "        # Concatenate features\n",
    "        concat_features = torch.cat([spatial_features, freq_features], dim=1)\n",
    "        \n",
    "        # Initial fusion\n",
    "        fused = self.conv1(concat_features)\n",
    "        \n",
    "        # Apply attention\n",
    "        fused = self.ca(fused)\n",
    "        fused = self.sa(fused)\n",
    "        \n",
    "        # Final processing\n",
    "        fused = self.conv2(fused)\n",
    "        fused = self.norm(fused)\n",
    "        fused = self.prelu(fused)\n",
    "        \n",
    "        return fused\n",
    "\n",
    "class UpscaleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upscaling block using pixel-shuffle\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, scale_factor):\n",
    "        super(UpscaleBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, in_channels * (scale_factor ** 2), 3, 1, 1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
    "        self.prelu = nn.PReLU(num_parameters=in_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        return self.prelu(x)\n",
    "\n",
    "class HybridFrequencySpatialNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Hybrid Network for frequency-spatial image restoration\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=3, base_channels=64, scale_factor=2):\n",
    "        super(HybridFrequencySpatialNetwork, self).__init__()\n",
    "        \n",
    "        # Feature extraction for both branches\n",
    "        self.feature_extract = nn.Conv2d(in_channels, base_channels, 3, 1, 1)\n",
    "        \n",
    "        # Branches\n",
    "        self.spatial_branch = SpatialBranch(base_channels, base_channels, \n",
    "                                           num_blocks=cfg.num_residual_blocks//2)\n",
    "        self.frequency_branch = FrequencyBranch(base_channels, base_channels)\n",
    "        \n",
    "        # Fusion\n",
    "        self.fusion = AttentionFusion(base_channels)\n",
    "        \n",
    "        # Deep feature extraction after fusion\n",
    "        self.deep_features = ResidualGroup(base_channels, n_blocks=cfg.num_residual_blocks, \n",
    "                                          use_ca=cfg.use_channel_attention, \n",
    "                                          use_sa=cfg.use_spatial_attention)\n",
    "        \n",
    "        # Reconstruction\n",
    "        self.reconstruction = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels, 3, 1, 1),\n",
    "            UpscaleBlock(base_channels, scale_factor),\n",
    "            nn.Conv2d(base_channels, out_channels, 3, 1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract initial features\n",
    "        initial_features = self.feature_extract(x)\n",
    "        \n",
    "        # Process through branches\n",
    "        spatial_features = self.spatial_branch(initial_features)\n",
    "        freq_features, complex_features = self.frequency_branch(initial_features)\n",
    "        \n",
    "        # Fuse features\n",
    "        fused_features = self.fusion(spatial_features, freq_features)\n",
    "        \n",
    "        # Deep feature processing\n",
    "        deep_features = self.deep_features(fused_features)\n",
    "        \n",
    "        # Reconstruction\n",
    "        output = self.reconstruction(deep_features)\n",
    "        \n",
    "        # Return output and intermediate features for additional supervision\n",
    "        return output, (spatial_features, freq_features, complex_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function in frequency domain\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FrequencyLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        # Convert to frequency domain\n",
    "        output_fft = torch.fft.fft2(output)\n",
    "        target_fft = torch.fft.fft2(target)\n",
    "        \n",
    "        # Compute magnitude difference\n",
    "        output_magnitude = torch.sqrt(output_fft.real**2 + output_fft.imag**2 + 1e-10)\n",
    "        target_magnitude = torch.sqrt(target_fft.real**2 + target_fft.imag**2 + 1e-10)\n",
    "        \n",
    "        # Apply log scaling\n",
    "        output_log_magnitude = torch.log(output_magnitude + 1.0)\n",
    "        target_log_magnitude = torch.log(target_magnitude + 1.0)\n",
    "        \n",
    "        # Compute L1 loss on log magnitude\n",
    "        magnitude_loss = F.l1_loss(output_log_magnitude, target_log_magnitude)\n",
    "        \n",
    "        # Compute phase difference\n",
    "        output_phase = torch.atan2(output_fft.imag, output_fft.real + 1e-10)\n",
    "        target_phase = torch.atan2(target_fft.imag, target_fft.real + 1e-10)\n",
    "        \n",
    "        # Phase loss (accounting for phase wrapping)\n",
    "        phase_diff = torch.abs(output_phase - target_phase)\n",
    "        phase_diff = torch.min(phase_diff, 2*np.pi - phase_diff)\n",
    "        phase_loss = phase_diff.mean()\n",
    "        \n",
    "        # Combined loss\n",
    "        return magnitude_loss + 0.5 * phase_loss\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceptual loss using VGG19 features\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_layers=[2, 7, 12, 21]):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        \n",
    "        # Load pre-trained VGG19\n",
    "        vgg = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()\n",
    "        \n",
    "        self.feature_extractors = nn.ModuleList()\n",
    "        self.feature_layers = feature_layers\n",
    "        \n",
    "        # Create feature extractors for each layer\n",
    "        for i in range(max(feature_layers) + 1):\n",
    "            self.feature_extractors.append(vgg[i])\n",
    "            \n",
    "            # Don't need gradients for VGG\n",
    "            if i in feature_layers:\n",
    "                for param in vgg[i].parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        loss = 0.0\n",
    "        \n",
    "        # Register hooks to get features\n",
    "        x_features = []\n",
    "        target_features = []\n",
    "        \n",
    "        # Extract features\n",
    "        for i, layer in enumerate(self.feature_extractors):\n",
    "            x = layer(x)\n",
    "            target = layer(target)\n",
    "            \n",
    "            if i in self.feature_layers:\n",
    "                loss += F.l1_loss(x, target)\n",
    "        \n",
    "        return loss / len(self.feature_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c9c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"Calculate PSNR between two images\"\"\"\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * math.log10(max_pixel / math.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    \"\"\"Calculate SSIM between two images using skimage implementation\"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if img1.shape[-1] == 3:\n",
    "        # Convert RGB to grayscale using standard weights\n",
    "        y1 = 0.299 * img1[:,:,0] + 0.587 * img1[:,:,1] + 0.114 * img1[:,:,2]\n",
    "        y2 = 0.299 * img2[:,:,0] + 0.587 * img2[:,:,1] + 0.114 * img2[:,:,2]\n",
    "        return ssim_metric(y1, y2, data_range=1.0)\n",
    "    else:\n",
    "        return ssim_metric(img1, img2, data_range=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, val_loader, device, epoch):\n",
    "    \"\"\"Visualize model results on validation set\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of validation images\n",
    "        batch = next(iter(val_loader))\n",
    "        lr_imgs = batch['lr'].to(device)\n",
    "        hr_imgs = batch['hr'].to(device)\n",
    "        \n",
    "        # Generate predictions\n",
    "        outputs, _ = model(lr_imgs)\n",
    "        \n",
    "        # Convert to numpy\n",
    "        lr_np = lr_imgs[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        sr_np = outputs[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        hr_np = hr_imgs[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Clip predictions\n",
    "        sr_np = np.clip(sr_np, 0, 1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        psnr = calculate_psnr(sr_np, hr_np)\n",
    "        ssim = calculate_ssim(sr_np, hr_np)\n",
    "        \n",
    "        # Plot results\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(lr_np)\n",
    "        plt.title('Low Resolution')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(sr_np)\n",
    "        plt.title(f'Super Resolution\\nPSNR: {psnr:.2f}, SSIM: {ssim:.4f}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(hr_np)\n",
    "        plt.title('High Resolution')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(cfg.save_dir, f'results_epoch_{epoch}.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Log to wandb if enabled\n",
    "        if cfg.use_wandb:\n",
    "            wandb.log({\n",
    "                'examples': wandb.Image(plt.gcf()),\n",
    "                'epoch': epoch\n",
    "            })\n",
    "\n",
    "def plot_training_curves(train_losses, val_psnrs):\n",
    "    \"\"\"Plot training curves\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_psnrs)\n",
    "    plt.title('Validation PSNR')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('PSNR (dB)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(cfg.save_dir, 'training_curves.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, val_psnr, save_path):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_psnr': val_psnr\n",
    "    }, save_path)\n",
    "\n",
    "def warmup_learning_rate(base_lr, iter, warmup_iter):\n",
    "    \"\"\"Warm up learning rate linearly\"\"\"\n",
    "    return base_lr * iter / warmup_iter\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Train the hybrid frequency-spatial image restoration model\"\"\"\n",
    "    # Create data loaders\n",
    "    train_dataset = DIV2KDataset(\n",
    "        cfg.data_root, \n",
    "        split='train', \n",
    "        scale=cfg.scale, \n",
    "        patch_size=cfg.patch_size,\n",
    "        augment=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = DIV2KDataset(\n",
    "        cfg.data_root, \n",
    "        split='valid', \n",
    "        scale=cfg.scale, \n",
    "        patch_size=cfg.patch_size,\n",
    "        augment=False\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=cfg.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=1, \n",
    "        shuffle=False, \n",
    "        num_workers=cfg.num_workers\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = HybridFrequencySpatialNetwork(\n",
    "        in_channels=3, \n",
    "        out_channels=3, \n",
    "        base_channels=cfg.base_channels, \n",
    "        scale_factor=cfg.scale\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define loss functions\n",
    "    l1_loss = nn.L1Loss().to(device)\n",
    "    perceptual_loss = PerceptualLoss().to(device)\n",
    "    frequency_loss = FrequencyLoss().to(device)\n",
    "    \n",
    "    # Define optimizer with weight decay\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=cfg.lr, \n",
    "        weight_decay=cfg.weight_decay\n",
    "    )\n",
    "    \n",
    "    # Define scheduler\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=cfg.num_epochs // 10, \n",
    "        T_mult=2, \n",
    "        eta_min=cfg.min_lr\n",
    "    )\n",
    "    \n",
    "    # Gradient scaler for mixed precision\n",
    "    scaler = GradScaler(enabled=cfg.use_amp)\n",
    "    \n",
    "    # Initialize statistics\n",
    "    best_val_psnr = 0.0\n",
    "    train_losses = []\n",
    "    val_psnrs = []\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    checkpoint_path = os.path.join(cfg.save_dir, 'best_model.pth')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_psnr = checkpoint['val_psnr']\n",
    "        print(f\"Resuming from epoch {start_epoch} with PSNR {best_val_psnr:.4f}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, cfg.num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{cfg.num_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Initialize progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.num_epochs}\")\n",
    "        \n",
    "        # Warm-up learning rate\n",
    "        if epoch < cfg.warmup_epochs:\n",
    "            # Warm-up learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = warmup_learning_rate(cfg.lr, epoch, cfg.warmup_epochs)\n",
    "        \n",
    "        # Track time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training loop\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Get data\n",
    "            lr_imgs = batch['lr'].to(device)\n",
    "            hr_imgs = batch['hr'].to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast(enabled=cfg.use_amp):\n",
    "                outputs, (spatial_features, freq_features, complex_features) = model(lr_imgs)\n",
    "                \n",
    "                # Calculate losses\n",
    "                loss_l1 = l1_loss(outputs, hr_imgs)\n",
    "                loss_perceptual = perceptual_loss(outputs, hr_imgs)\n",
    "                loss_frequency = frequency_loss(outputs, hr_imgs)\n",
    "                \n",
    "                # Combined loss\n",
    "                loss = (cfg.l1_weight * loss_l1 + \n",
    "                       cfg.perceptual_weight * loss_perceptual + \n",
    "                       cfg.freq_loss_weight * loss_frequency)\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update progress bar\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Log to wandb if enabled\n",
    "            if cfg.use_wandb and batch_idx % cfg.log_interval == 0:\n",
    "                wandb.log({\n",
    "                    'train/loss': loss.item(),\n",
    "                    'train/l1_loss': loss_l1.item(),\n",
    "                    'train/perceptual_loss': loss_perceptual.item(),\n",
    "                    'train/frequency_loss': loss_frequency.item(),\n",
    "                    'train/learning_rate': optimizer.param_groups[0]['lr']\n",
    "                })\n",
    "        \n",
    "        # Calculate average epoch loss\n",
    "        epoch_loss /= len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_psnr = 0.0\n",
    "        val_ssim = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                lr_imgs = batch['lr'].to(device)\n",
    "                hr_imgs = batch['hr'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs, _ = model(lr_imgs)\n",
    "                \n",
    "                # Convert to numpy for metric calculation\n",
    "                output = outputs[0].cpu().numpy().transpose(1, 2, 0)\n",
    "                target = hr_imgs[0].cpu().numpy().transpose(1, 2, 0)\n",
    "                \n",
    "                # Clip predictions to valid range\n",
    "                output = np.clip(output, 0, 1)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                psnr = calculate_psnr(output, target)\n",
    "                ssim = calculate_ssim(output, target)\n",
    "                \n",
    "                val_psnr += psnr\n",
    "                val_ssim += ssim\n",
    "        \n",
    "        # Calculate average validation metrics\n",
    "        val_psnr /= len(val_loader)\n",
    "        val_ssim /= len(val_loader)\n",
    "        val_psnrs.append(val_psnr)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{cfg.num_epochs} - \"\n",
    "              f\"Train Loss: {epoch_loss:.4f} - \"\n",
    "              f\"Val PSNR: {val_psnr:.4f} - \"\n",
    "              f\"Val SSIM: {val_ssim:.4f}\")\n",
    "        \n",
    "        # Log to wandb if enabled\n",
    "        if cfg.use_wandb:\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train/epoch_loss': epoch_loss,\n",
    "                'val/psnr': val_psnr,\n",
    "                'val/ssim': val_ssim,\n",
    "                'train/learning_rate': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        \n",
    "        # Save checkpoint if validation PSNR improved\n",
    "        if val_psnr > best_val_psnr:\n",
    "            best_val_psnr = val_psnr\n",
    "            save_checkpoint(\n",
    "                model, optimizer, scheduler, epoch, val_psnr,\n",
    "                os.path.join(cfg.save_dir, 'best_model.pth')\n",
    "            )\n",
    "            print(f\"New best model saved with PSNR: {val_psnr:.4f}\")\n",
    "        \n",
    "        # Save periodic checkpoint\n",
    "        if (epoch + 1) % cfg.checkpoint_interval == 0:\n",
    "            save_checkpoint(\n",
    "                model, optimizer, scheduler, epoch, val_psnr,\n",
    "                os.path.join(cfg.save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "            )\n",
    "        \n",
    "        # Visualize some results\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            visualize_results(model, val_loader, device, epoch + 1)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if len(val_psnrs) > 10:\n",
    "            if val_psnr < max(val_psnrs[-10:]):\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # Save final model\n",
    "    save_checkpoint(\n",
    "        model, optimizer, scheduler, cfg.num_epochs, val_psnr,\n",
    "        os.path.join(cfg.save_dir, 'final_model.pth')\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves(train_losses, val_psnrs)\n",
    "    \n",
    "    # Cleanup\n",
    "    if cfg.use_wandb:\n",
    "        wandb.finish()\n",
    "    \n",
    "    return model, train_losses, val_psnrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run training\"\"\"\n",
    "    # Create save directory\n",
    "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize wandb if enabled\n",
    "    if cfg.use_wandb:\n",
    "        wandb.init(\n",
    "            project=\"hybrid-freq-spatial-image-restoration\",\n",
    "            config=vars(cfg),\n",
    "            name=f\"scale{cfg.scale}_channels{cfg.base_channels}\"\n",
    "        )\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_psnrs = train_model()\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Best validation PSNR: {max(val_psnrs):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
